{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import ggcnn.experiment as experiment\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def load_sa1_dataset():\n",
    "    keys_SA1 = []\n",
    "    features_SA1 = []\n",
    "    labels = []\n",
    "    keys_SA2 = []\n",
    "    features_SA2 = []\n",
    "    \n",
    "    # Load SA1 Node Features\n",
    "    with open('Data/2018-09-02-NSW-SA1Input-Normalised.csv', 'r') as file:\n",
    "        for i, line in enumerate(file):\n",
    "            if i == 0:  # Skip first line (header)\n",
    "                continue\n",
    "            s = line[:-1].split(',')  # Last value in line is \\n\n",
    "            keys_SA1.append(s[0])\n",
    "            features_SA1.extend([float(v) for v in s[1:-1]])  # Last column is the outcome y\n",
    "#             labels.append(np.floor(float(s[-1]) / 10).astype(int))\n",
    "            labels.append(float(s[-1]))\n",
    "    \n",
    "    \n",
    "    # Load SA2 Node Features\n",
    "    with open('Data/2018-08-28-NSW-SA2Input-Normalised.csv', 'r') as file:\n",
    "        for i, line in enumerate(file):\n",
    "            if i == 0:  # Skip first line (header)\n",
    "                continue\n",
    "            s = line[:-1].split(',')  # Last value in line is \\n\n",
    "            keys_SA2.append(s[0])\n",
    "            features_SA2.extend([float(v) for v in s[1:-1]])  # Last column is the outcome y\n",
    "\n",
    "    labels = np.array(labels)\n",
    "    features_SA1 = np.array(features_SA1).reshape((len(keys_SA1), -1))\n",
    "    features_SA2 = np.array(features_SA2).reshape((len(keys_SA2), -1))\n",
    "    \n",
    "    # Load SA1 Link Features\n",
    "    with open('Data/Geography/2018-09-01-NSW-Neighbouring_Suburbs_With_Bridges-Filtered.csv', 'r') as file:\n",
    "        adj_mat_SA1 = np.zeros((len(keys_SA1), len(keys_SA1)))\n",
    "        for i, line in enumerate(file):\n",
    "            if i == 0:  # Skip first line (header)\n",
    "                continue\n",
    "            s = line[:-1].split(',')\n",
    "            a = keys_SA1.index(s[0])\n",
    "            b = keys_SA1.index(s[1])\n",
    "            adj_mat_SA1[a, b] = 1\n",
    "            adj_mat_SA1[b, a] = 1\n",
    "    \n",
    "\n",
    "    # Load SA2 Link Features\n",
    "    with open('Data/Geography/2018-09-01-SA2Neighbouring_Suburbs_With_Bridges-Filtered.csv', 'r') as file:\n",
    "        adj_mat_SA2 = np.zeros((len(keys_SA2), len(keys_SA2)))\n",
    "        for i, line in enumerate(file):\n",
    "            if i == 0:  # Skip first line (header)\n",
    "                continue\n",
    "            s = line[:-1].split(',')\n",
    "            a = keys_SA2.index(s[0])\n",
    "            b = keys_SA2.index(s[1])\n",
    "            adj_mat_SA2[a, b] = 1\n",
    "            adj_mat_SA2[b, a] = 1\n",
    "    \n",
    "    \n",
    "    # Load SA1, SA2 Links\n",
    "    with open('Data/2018-09-02-SA1SA2Links.csv', 'r') as file:\n",
    "        adj_mat_SA1SA2 = np.zeros((len(keys_SA1), len(keys_SA2)))\n",
    "        for i, line in enumerate(file):\n",
    "            if i == 0:  # Skip first line (header)\n",
    "                continue\n",
    "            s = line[:-1].split(',')\n",
    "            a = keys_SA1.index(s[0])\n",
    "            b = keys_SA2.index(s[1])\n",
    "            adj_mat_SA1SA2[a, b] = 1\n",
    "    \n",
    "    adj_mat_SA2SA1 = np.transpose(adj_mat_SA1SA2)\n",
    "    \n",
    "    return (features_SA1, adj_mat_SA1, labels, features_SA2, adj_mat_SA2, adj_mat_SA1SA2, adj_mat_SA2SA1), (keys_SA1, keys_SA2)\n",
    "\n",
    "dataset, keys = load_sa1_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SA1Experiment():\n",
    "    def __init__(self, neurons, blocks, reverseLinkagePosition = \"Early\", linkagePosition = \"Late\",\n",
    "                    linkageActFun = True, linkageBatchNorm = True, linkageNeurons = None,\n",
    "                    auxilaryEmbedding1 = False, auxilaryEmbedding2 = False,\n",
    "                    auxilaryGraph = False, linkage_adjustment_components = None):\n",
    "        self.blocks = blocks\n",
    "        self.neurons = neurons\n",
    "        self.reverseLinkagePosition = reverseLinkagePosition\n",
    "        self.linkagePosition = linkagePosition\n",
    "        self.linkageActFun = linkageActFun\n",
    "        self.linkageBatchNorm = linkageBatchNorm\n",
    "        self.linkageNeurons = linkageNeurons\n",
    "        self.auxilaryEmbedding1 = auxilaryEmbedding1\n",
    "        self.auxilaryEmbedding2 = auxilaryEmbedding2\n",
    "        self.auxilaryGraph = auxilaryGraph\n",
    "        self.linkage_adjustment_components = linkage_adjustment_components\n",
    "    \n",
    "    def create_network(self, net, input):\n",
    "        net.create_network(input)\n",
    "\n",
    "        if self.linkage_adjustment_components is not None:\n",
    "            net.make_linkage_adjustment_layer()\n",
    "        \n",
    "        if self.reverseLinkagePosition == \"Early\" or self.reverseLinkagePosition == \"Both\":\n",
    "            net.make_reverse_auxilary_linkage_layer(self.linkageNeurons, with_act_func = self.linkageActFun, with_bn = self.linkageBatchNorm)\n",
    "        if self.linkagePosition == \"Early\" or self.linkagePosition == \"Both\":\n",
    "            net.make_auxilary_linkage_layer(self.linkageNeurons, with_act_func = self.linkageActFun, with_bn = self.linkageBatchNorm)\n",
    "        \n",
    "        net.make_embedding_layer(self.neurons)\n",
    "        net.make_dropout_layer()\n",
    "        \n",
    "        for _ in range(self.blocks):\n",
    "            net.make_graphcnn_layer(self.neurons)\n",
    "            net.make_dropout_layer()\n",
    "            net.make_embedding_layer(self.neurons)\n",
    "            net.make_dropout_layer()\n",
    "        \n",
    "        if self.auxilaryEmbedding1:\n",
    "            net.make_auxilary_embedding_layer(self.neurons)\n",
    "            net.make_dropout_layer(input_type = 'current_V_auxilary')\n",
    "        if self.reverseLinkagePosition == \"Late\" or self.reverseLinkagePosition == \"Both\":\n",
    "            net.make_reverse_auxilary_linkage_layer(self.linkageNeurons, with_act_func = self.linkageActFun, with_bn = self.linkageBatchNorm)\n",
    "        if self.auxilaryEmbedding2:\n",
    "            net.make_auxilary_embedding_layer(self.neurons)\n",
    "            net.make_dropout_layer(input_type = 'current_V_auxilary')\n",
    "        if self.auxilaryGraph:\n",
    "            net.make_auxilary_graphcnn_layer(self.neurons)\n",
    "            net.make_dropout_layer(input_type = 'current_V_auxilary')\n",
    "        if self.linkagePosition == \"Late\" or self.linkagePosition == \"Both\":\n",
    "            net.make_auxilary_linkage_layer(self.linkageNeurons, with_act_func = self.linkageActFun, with_bn = self.linkageBatchNorm)\n",
    "        \n",
    "        net.make_embedding_layer(self.neurons)\n",
    "        net.make_embedding_layer(1, name='final', with_bn=False, with_act_func = False)\n",
    "\n",
    "\n",
    "######\n",
    "\n",
    "def run(no_folds = 5, supervised = True, i = 0, l = 2, n = 128, expParameters = {}):\n",
    "    inst = KFold(n_splits = no_folds, shuffle=True, random_state=125)\n",
    "        \n",
    "    exp = experiment.GGCNNExperiment('2018-08-28-SA1SA2', '2018-08-28-SA1SA2', SA1Experiment(neurons = n, blocks = l, **expParameters))\n",
    "\n",
    "    exp.num_iterations = 5000\n",
    "    exp.optimizer = 'adam'\n",
    "    exp.loss_type = 'linear'\n",
    "\n",
    "    exp.debug = True  # Was True\n",
    "\n",
    "    exp.preprocess_data(dataset)\n",
    "\n",
    "    valid_idx = np.flatnonzero(dataset[2] >= 0)  # Missing data labelled with -1\n",
    "    if supervised:\n",
    "        train_idx, test_idx = list(inst.split( valid_idx ))[i]\n",
    "    else:\n",
    "        test_idx, train_idx = list(inst.split( valid_idx ))[i]  # Reversed to get more samples in the test set than the training set\n",
    "\n",
    "    n_components = expParameters.get('linkage_adjustment_components', None)\n",
    "    exp.create_data(train_idx, test_idx, n_components = n_components)\n",
    "    exp.build_network()\n",
    "    results = exp.run()\n",
    "    \n",
    "    # Node type of input nodes: 0 = training set; 1 = test set; -1 = neither\n",
    "    idx_split = np.empty((len(dataset[2]), 1))\n",
    "    idx_split.fill(-1)\n",
    "    idx_split[train_idx] = 0\n",
    "    idx_split[test_idx] = 1\n",
    "\n",
    "    return results, idx_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-09-02 22:40:14.344590 Creating training Tensorflow Tensors\n",
      "PCA variance ratio:  [0.17138442 0.12530963 0.08620865]\n",
      "2018-09-02 22:40:15.399994 Creating training network\n",
      "2018-09-02 22:40:16.032917 Creating loss function and summaries\n",
      "2018-09-02 22:40:16.064039 Training model \"2018-08-28-SA1SA2\"!\n",
      "2018-09-02 22:40:16.064098 Preparing training\n",
      "2018-09-02 22:40:19.418936 Starting threads\n",
      "2018-09-02 22:40:19.419568 Starting training. train_batch_size: 0 test_batch_size: 0\n",
      "2018-09-02 22:40:22.342929 Test Step 0 Finished\n",
      "2018-09-02 22:40:22.343017 Test Step 0 \"min loss\" =  9.735105e+16\n",
      "2018-09-02 22:40:22.343062 Test Step 0 \"loss\" =  9.735105e+16\n",
      "2018-09-02 22:40:27.006720 Training Step 0 Finished Timing (Training: 0.614677, Test: 0.385299) after 7.58708 seconds\n",
      "2018-09-02 22:40:27.006834 Training Step 0 \"min loss\" =  2555.4531\n",
      "2018-09-02 22:40:27.006883 Training Step 0 \"loss\" =  2555.4531\n",
      "2018-09-02 22:40:38.281351 Test Step 5 Finished\n",
      "2018-09-02 22:40:38.281438 Test Step 5 \"min loss\" =  2452.7302\n",
      "2018-09-02 22:40:38.281477 Test Step 5 \"loss\" =  2452.7302\n",
      "2018-09-02 22:40:40.897466 Training Step 5 Finished Timing (Training: 0.91868, Test: 0.0813069) after 13.8905 seconds\n",
      "2018-09-02 22:40:40.897709 Training Step 5 \"min loss\" =  2500.313\n",
      "2018-09-02 22:40:40.897751 Training Step 5 \"loss\" =  2500.313\n",
      "2018-09-02 22:40:52.363908 Test Step 10 Finished\n",
      "2018-09-02 22:40:52.364043 Test Step 10 \"min loss\" =  2414.3997\n",
      "2018-09-02 22:40:52.364117 Test Step 10 \"loss\" =  2414.3997\n",
      "2018-09-02 22:40:54.954616 Training Step 10 Finished Timing (Training: 0.919734, Test: 0.0802367) after 14.0568 seconds\n",
      "2018-09-02 22:40:54.954715 Training Step 10 \"min loss\" =  2449.1716\n",
      "2018-09-02 22:40:54.954760 Training Step 10 \"loss\" =  2449.1716\n",
      "2018-09-02 22:41:06.970850 Test Step 15 Finished\n",
      "2018-09-02 22:41:06.970941 Test Step 15 \"min loss\" =  2363.3208\n",
      "2018-09-02 22:41:06.970982 Test Step 15 \"loss\" =  2363.3208\n",
      "2018-09-02 22:41:09.634607 Training Step 15 Finished Timing (Training: 0.920117, Test: 0.0798546) after 14.6798 seconds\n",
      "2018-09-02 22:41:09.634699 Training Step 15 \"min loss\" =  2401.2556\n",
      "2018-09-02 22:41:09.634741 Training Step 15 \"loss\" =  2401.2556\n",
      "2018-09-02 22:41:22.328019 Test Step 20 Finished\n",
      "2018-09-02 22:41:22.328126 Test Step 20 \"min loss\" =  2298.8276\n",
      "2018-09-02 22:41:22.328175 Test Step 20 \"loss\" =  2298.8276\n",
      "2018-09-02 22:41:25.292861 Training Step 20 Finished Timing (Training: 0.917209, Test: 0.0827631) after 15.6581 seconds\n",
      "2018-09-02 22:41:25.292959 Training Step 20 \"min loss\" =  2357.897\n",
      "2018-09-02 22:41:25.293183 Training Step 20 \"loss\" =  2357.897\n",
      "2018-09-02 22:41:37.497706 Test Step 25 Finished\n",
      "2018-09-02 22:41:37.497797 Test Step 25 \"min loss\" =  2234.2737\n",
      "2018-09-02 22:41:37.497835 Test Step 25 \"loss\" =  2234.2737\n",
      "2018-09-02 22:41:40.373435 Training Step 25 Finished Timing (Training: 0.918526, Test: 0.0814441) after 15.0802 seconds\n",
      "2018-09-02 22:41:40.373599 Training Step 25 \"min loss\" =  2313.8296\n",
      "2018-09-02 22:41:40.373694 Training Step 25 \"loss\" =  2313.8296\n",
      "2018-09-02 22:41:52.839262 Test Step 30 Finished\n",
      "2018-09-02 22:41:52.839353 Test Step 30 \"min loss\" =  2179.2627\n",
      "2018-09-02 22:41:52.839396 Test Step 30 \"loss\" =  2179.2627\n",
      "2018-09-02 22:41:55.612380 Training Step 30 Finished Timing (Training: 0.919293, Test: 0.080676) after 15.2386 seconds\n",
      "2018-09-02 22:41:55.612476 Training Step 30 \"min loss\" =  2270.7126\n",
      "2018-09-02 22:41:55.612524 Training Step 30 \"loss\" =  2270.7126\n",
      "2018-09-02 22:42:08.169785 Test Step 35 Finished\n",
      "2018-09-02 22:42:08.170035 Test Step 35 \"min loss\" =  2128.409\n",
      "2018-09-02 22:42:08.170073 Test Step 35 \"loss\" =  2128.409\n",
      "2018-09-02 22:42:11.020101 Training Step 35 Finished Timing (Training: 0.919911, Test: 0.0800559) after 15.4075 seconds\n",
      "2018-09-02 22:42:11.020209 Training Step 35 \"min loss\" =  2226.3467\n",
      "2018-09-02 22:42:11.020260 Training Step 35 \"loss\" =  2226.3467\n",
      "2018-09-02 22:42:23.608495 Test Step 40 Finished\n",
      "2018-09-02 22:42:23.608653 Test Step 40 \"min loss\" =  2080.1143\n",
      "2018-09-02 22:42:23.608736 Test Step 40 \"loss\" =  2080.1143\n",
      "2018-09-02 22:42:26.300310 Training Step 40 Finished Timing (Training: 0.92027, Test: 0.0796965) after 15.28 seconds\n",
      "2018-09-02 22:42:26.300415 Training Step 40 \"min loss\" =  2185.6646\n",
      "2018-09-02 22:42:26.300465 Training Step 40 \"loss\" =  2185.6646\n",
      "2018-09-02 22:42:39.122452 Test Step 45 Finished\n",
      "2018-09-02 22:42:39.122592 Test Step 45 \"min loss\" =  2049.1396\n",
      "2018-09-02 22:42:39.122664 Test Step 45 \"loss\" =  2049.1396\n",
      "2018-09-02 22:42:41.846323 Training Step 45 Finished Timing (Training: 0.919962, Test: 0.0800042) after 15.5458 seconds\n",
      "2018-09-02 22:42:41.846419 Training Step 45 \"min loss\" =  2139.9463\n",
      "2018-09-02 22:42:41.846466 Training Step 45 \"loss\" =  2139.9463\n",
      "2018-09-02 22:42:54.685573 Test Step 50 Finished\n",
      "2018-09-02 22:42:54.685660 Test Step 50 \"min loss\" =  2016.1503\n",
      "2018-09-02 22:42:54.685709 Test Step 50 \"loss\" =  2016.1503\n",
      "2018-09-02 22:42:57.370439 Training Step 50 Finished Timing (Training: 0.919693, Test: 0.0802742) after 15.5239 seconds\n",
      "2018-09-02 22:42:57.370538 Training Step 50 \"min loss\" =  2100.7063\n",
      "2018-09-02 22:42:57.370584 Training Step 50 \"loss\" =  2100.7063\n",
      "2018-09-02 22:43:10.464862 Test Step 55 Finished\n",
      "2018-09-02 22:43:10.464956 Test Step 55 \"min loss\" =  1966.6818\n",
      "2018-09-02 22:43:10.465001 Test Step 55 \"loss\" =  1966.6818\n",
      "2018-09-02 22:43:13.186310 Training Step 55 Finished Timing (Training: 0.919751, Test: 0.0802174) after 15.8157 seconds\n",
      "2018-09-02 22:43:13.186402 Training Step 55 \"min loss\" =  2055.2605\n",
      "2018-09-02 22:43:13.186445 Training Step 55 \"loss\" =  2055.2605\n",
      "2018-09-02 22:43:25.636760 Test Step 60 Finished\n",
      "2018-09-02 22:43:25.636866 Test Step 60 \"min loss\" =  1922.1063\n",
      "2018-09-02 22:43:25.636917 Test Step 60 \"loss\" =  1922.1063\n",
      "2018-09-02 22:43:28.279358 Training Step 60 Finished Timing (Training: 0.919404, Test: 0.0805625) after 15.0929 seconds\n",
      "2018-09-02 22:43:28.279459 Training Step 60 \"min loss\" =  2016.4835\n",
      "2018-09-02 22:43:28.279508 Training Step 60 \"loss\" =  2016.4835\n",
      "2018-09-02 22:43:40.538449 Test Step 65 Finished\n",
      "2018-09-02 22:43:40.538540 Test Step 65 \"min loss\" =  1879.3973\n",
      "2018-09-02 22:43:40.538590 Test Step 65 \"loss\" =  1879.3973\n",
      "2018-09-02 22:43:43.203937 Training Step 65 Finished Timing (Training: 0.919041, Test: 0.0809267) after 14.9244 seconds\n",
      "2018-09-02 22:43:43.204034 Training Step 65 \"min loss\" =  1976.6781\n",
      "2018-09-02 22:43:43.204080 Training Step 65 \"loss\" =  1976.6781\n",
      "2018-09-02 22:43:56.105824 Test Step 70 Finished\n",
      "2018-09-02 22:43:56.106111 Test Step 70 \"min loss\" =  1850.3916\n",
      "2018-09-02 22:43:56.106435 Test Step 70 \"loss\" =  1850.3916\n",
      "2018-09-02 22:43:58.880846 Training Step 70 Finished Timing (Training: 0.919427, Test: 0.0805386) after 15.6767 seconds\n",
      "2018-09-02 22:43:58.880949 Training Step 70 \"min loss\" =  1937.9434\n",
      "2018-09-02 22:43:58.881001 Training Step 70 \"loss\" =  1937.9434\n",
      "2018-09-02 22:44:11.082067 Test Step 75 Finished\n",
      "2018-09-02 22:44:11.082168 Test Step 75 \"min loss\" =  1815.3016\n",
      "2018-09-02 22:44:11.082208 Test Step 75 \"loss\" =  1815.3016\n",
      "2018-09-02 22:44:13.959744 Training Step 75 Finished Timing (Training: 0.919704, Test: 0.0802619) after 15.0787 seconds\n",
      "2018-09-02 22:44:13.959847 Training Step 75 \"min loss\" =  1897.4851\n",
      "2018-09-02 22:44:13.959898 Training Step 75 \"loss\" =  1897.4851\n",
      "2018-09-02 22:44:26.284574 Test Step 80 Finished\n",
      "2018-09-02 22:44:26.284909 Test Step 80 \"min loss\" =  1790.1118\n",
      "2018-09-02 22:44:26.284959 Test Step 80 \"loss\" =  1790.1118\n",
      "2018-09-02 22:44:29.193191 Training Step 80 Finished Timing (Training: 0.918867, Test: 0.0810986) after 15.2332 seconds\n",
      "2018-09-02 22:44:29.193291 Training Step 80 \"min loss\" =  1858.3547\n",
      "2018-09-02 22:44:29.193338 Training Step 80 \"loss\" =  1858.3547\n",
      "2018-09-02 22:44:41.187256 Test Step 85 Finished\n",
      "2018-09-02 22:44:41.187353 Test Step 85 \"min loss\" =  1757.8514\n",
      "2018-09-02 22:44:41.187395 Test Step 85 \"loss\" =  1757.8514\n",
      "2018-09-02 22:44:44.014073 Training Step 85 Finished Timing (Training: 0.919097, Test: 0.0808674) after 14.8207 seconds\n",
      "2018-09-02 22:44:44.014243 Training Step 85 \"min loss\" =  1817.8214\n",
      "2018-09-02 22:44:44.014346 Training Step 85 \"loss\" =  1817.8214\n",
      "2018-09-02 22:44:56.937551 Test Step 90 Finished\n",
      "2018-09-02 22:44:56.937644 Test Step 90 \"min loss\" =  1721.7487\n",
      "2018-09-02 22:44:56.937698 Test Step 90 \"loss\" =  1721.7487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-09-02 22:44:59.987267 Training Step 90 Finished Timing (Training: 0.91938, Test: 0.0805845) after 15.9728 seconds\n",
      "2018-09-02 22:44:59.987367 Training Step 90 \"min loss\" =  1778.7964\n",
      "2018-09-02 22:44:59.987417 Training Step 90 \"loss\" =  1778.7964\n",
      "2018-09-02 22:45:12.236280 Test Step 95 Finished\n",
      "2018-09-02 22:45:12.236370 Test Step 95 \"min loss\" =  1682.3273\n",
      "2018-09-02 22:45:12.236406 Test Step 95 \"loss\" =  1682.3273\n",
      "2018-09-02 22:45:14.969905 Training Step 95 Finished Timing (Training: 0.919571, Test: 0.0803935) after 14.9824 seconds\n",
      "2018-09-02 22:45:14.970003 Training Step 95 \"min loss\" =  1739.9954\n",
      "2018-09-02 22:45:14.970052 Training Step 95 \"loss\" =  1739.9954\n",
      "2018-09-02 22:45:27.454303 Test Step 100 Finished\n",
      "2018-09-02 22:45:27.454396 Test Step 100 \"min loss\" =  1654.0144\n",
      "2018-09-02 22:45:27.454703 Test Step 100 \"loss\" =  1654.0144\n",
      "2018-09-02 22:45:30.236512 Training Step 100 Finished Timing (Training: 0.919757, Test: 0.0802073) after 15.2664 seconds\n",
      "2018-09-02 22:45:30.236650 Training Step 100 \"min loss\" =  1700.8615\n",
      "2018-09-02 22:45:30.236730 Training Step 100 \"loss\" =  1700.8615\n",
      "2018-09-02 22:45:43.402031 Test Step 105 Finished\n",
      "2018-09-02 22:45:43.402270 Test Step 105 \"min loss\" =  1619.2545\n",
      "2018-09-02 22:45:43.402327 Test Step 105 \"loss\" =  1619.2545\n",
      "2018-09-02 22:45:46.042377 Training Step 105 Finished Timing (Training: 0.921581, Test: 0.0783959) after 15.8056 seconds\n",
      "2018-09-02 22:45:46.042477 Training Step 105 \"min loss\" =  1657.8353\n",
      "2018-09-02 22:45:46.042523 Training Step 105 \"loss\" =  1657.8353\n",
      "2018-09-02 22:45:58.588829 Test Step 110 Finished\n",
      "2018-09-02 22:45:58.588927 Test Step 110 \"min loss\" =  1561.404\n",
      "2018-09-02 22:45:58.588969 Test Step 110 \"loss\" =  1561.404\n",
      "2018-09-02 22:46:01.318020 Training Step 110 Finished Timing (Training: 0.919636, Test: 0.080339) after 15.2754 seconds\n",
      "2018-09-02 22:46:01.318283 Training Step 110 \"min loss\" =  1618.0872\n",
      "2018-09-02 22:46:01.318406 Training Step 110 \"loss\" =  1618.0872\n",
      "2018-09-02 22:46:14.287723 Test Step 115 Finished\n",
      "2018-09-02 22:46:14.287821 Test Step 115 \"min loss\" =  1533.719\n",
      "2018-09-02 22:46:14.287866 Test Step 115 \"loss\" =  1533.719\n",
      "2018-09-02 22:46:16.879059 Training Step 115 Finished Timing (Training: 0.918008, Test: 0.0819614) after 15.5606 seconds\n",
      "2018-09-02 22:46:16.879161 Training Step 115 \"min loss\" =  1577.8931\n",
      "2018-09-02 22:46:16.879208 Training Step 115 \"loss\" =  1577.8931\n",
      "2018-09-02 22:46:29.369648 Test Step 120 Finished\n",
      "2018-09-02 22:46:29.369743 Test Step 120 \"min loss\" =  1501.4731\n",
      "2018-09-02 22:46:29.369783 Test Step 120 \"loss\" =  1501.4731\n",
      "2018-09-02 22:46:32.080612 Training Step 120 Finished Timing (Training: 0.917688, Test: 0.0822828) after 15.2014 seconds\n",
      "2018-09-02 22:46:32.080710 Training Step 120 \"min loss\" =  1540.0906\n",
      "2018-09-02 22:46:32.080753 Training Step 120 \"loss\" =  1540.0906\n",
      "2018-09-02 22:46:44.321970 Test Step 125 Finished\n",
      "2018-09-02 22:46:44.322079 Test Step 125 \"min loss\" =  1463.3574\n",
      "2018-09-02 22:46:44.322165 Test Step 125 \"loss\" =  1463.3574\n",
      "2018-09-02 22:46:47.225037 Training Step 125 Finished Timing (Training: 0.916309, Test: 0.0836613) after 15.1442 seconds\n",
      "2018-09-02 22:46:47.225134 Training Step 125 \"min loss\" =  1499.3541\n",
      "2018-09-02 22:46:47.225179 Training Step 125 \"loss\" =  1499.3541\n",
      "2018-09-02 22:46:59.642935 Test Step 130 Finished\n",
      "2018-09-02 22:46:59.643278 Test Step 130 \"min loss\" =  1422.7916\n",
      "2018-09-02 22:46:59.643323 Test Step 130 \"loss\" =  1422.7916\n",
      "2018-09-02 22:47:02.682934 Training Step 130 Finished Timing (Training: 0.916106, Test: 0.0838621) after 15.4577 seconds\n",
      "2018-09-02 22:47:02.683017 Training Step 130 \"min loss\" =  1457.2126\n",
      "2018-09-02 22:47:02.683053 Training Step 130 \"loss\" =  1457.2126\n",
      "2018-09-02 22:47:14.984306 Test Step 135 Finished\n",
      "2018-09-02 22:47:14.984406 Test Step 135 \"min loss\" =  1385.9381\n",
      "2018-09-02 22:47:14.984447 Test Step 135 \"loss\" =  1385.9381\n",
      "2018-09-02 22:47:17.857229 Training Step 135 Finished Timing (Training: 0.916584, Test: 0.0833835) after 15.174 seconds\n",
      "2018-09-02 22:47:17.857335 Training Step 135 \"min loss\" =  1417.6942\n",
      "2018-09-02 22:47:17.857386 Training Step 135 \"loss\" =  1417.6942\n",
      "2018-09-02 22:47:30.122968 Test Step 140 Finished\n",
      "2018-09-02 22:47:30.123084 Test Step 140 \"min loss\" =  1342.1786\n",
      "2018-09-02 22:47:30.123146 Test Step 140 \"loss\" =  1342.1786\n",
      "2018-09-02 22:47:33.161202 Training Step 140 Finished Timing (Training: 0.917319, Test: 0.0826488) after 15.3038 seconds\n",
      "2018-09-02 22:47:33.161310 Training Step 140 \"min loss\" =  1380.911\n",
      "2018-09-02 22:47:33.161358 Training Step 140 \"loss\" =  1380.911\n",
      "2018-09-02 22:47:45.412295 Test Step 145 Finished\n",
      "2018-09-02 22:47:45.412407 Test Step 145 \"min loss\" =  1310.926\n",
      "2018-09-02 22:47:45.412475 Test Step 145 \"loss\" =  1310.926\n",
      "2018-09-02 22:47:48.193887 Training Step 145 Finished Timing (Training: 0.917962, Test: 0.082006) after 15.0325 seconds\n",
      "2018-09-02 22:47:48.194040 Training Step 145 \"min loss\" =  1336.5665\n",
      "2018-09-02 22:47:48.194152 Training Step 145 \"loss\" =  1336.5665\n",
      "2018-09-02 22:48:00.337160 Test Step 150 Finished\n",
      "2018-09-02 22:48:00.337249 Test Step 150 \"min loss\" =  1279.965\n",
      "2018-09-02 22:48:00.337291 Test Step 150 \"loss\" =  1279.965\n",
      "2018-09-02 22:48:02.932944 Training Step 150 Finished Timing (Training: 0.918384, Test: 0.0815835) after 14.7387 seconds\n",
      "2018-09-02 22:48:02.933041 Training Step 150 \"min loss\" =  1295.8999\n",
      "2018-09-02 22:48:02.933087 Training Step 150 \"loss\" =  1295.8999\n",
      "2018-09-02 22:48:15.401996 Test Step 155 Finished\n",
      "2018-09-02 22:48:15.402363 Test Step 155 \"min loss\" =  1244.8114\n",
      "2018-09-02 22:48:15.402493 Test Step 155 \"loss\" =  1244.8114\n",
      "2018-09-02 22:48:18.277649 Training Step 155 Finished Timing (Training: 0.918095, Test: 0.0818711) after 15.3445 seconds\n",
      "2018-09-02 22:48:18.277746 Training Step 155 \"min loss\" =  1256.8785\n",
      "2018-09-02 22:48:18.277793 Training Step 155 \"loss\" =  1256.8785\n",
      "2018-09-02 22:48:30.737157 Test Step 160 Finished\n",
      "2018-09-02 22:48:30.737275 Test Step 160 \"min loss\" =  1201.4358\n",
      "2018-09-02 22:48:30.737338 Test Step 160 \"loss\" =  1201.4358\n",
      "2018-09-02 22:48:33.761168 Training Step 160 Finished Timing (Training: 0.9183, Test: 0.0816661) after 15.4833 seconds\n",
      "2018-09-02 22:48:33.761285 Training Step 160 \"min loss\" =  1219.3494\n",
      "2018-09-02 22:48:33.761348 Training Step 160 \"loss\" =  1219.3494\n",
      "2018-09-02 22:48:46.094846 Test Step 165 Finished\n",
      "2018-09-02 22:48:46.094950 Test Step 165 \"min loss\" =  1181.4581\n",
      "2018-09-02 22:48:46.095005 Test Step 165 \"loss\" =  1181.4581\n",
      "2018-09-02 22:48:49.073582 Training Step 165 Finished Timing (Training: 0.918527, Test: 0.0814398) after 15.3122 seconds\n",
      "2018-09-02 22:48:49.073680 Training Step 165 \"min loss\" =  1175.4294\n",
      "2018-09-02 22:48:49.073726 Training Step 165 \"loss\" =  1175.4294\n",
      "2018-09-02 22:49:01.462856 Test Step 170 Finished\n",
      "2018-09-02 22:49:01.462953 Test Step 170 \"min loss\" =  1145.3077\n",
      "2018-09-02 22:49:01.462996 Test Step 170 \"loss\" =  1145.3077\n",
      "2018-09-02 22:49:04.241479 Training Step 170 Finished Timing (Training: 0.918946, Test: 0.081021) after 15.1677 seconds\n",
      "2018-09-02 22:49:04.241586 Training Step 170 \"min loss\" =  1136.8573\n",
      "2018-09-02 22:49:04.241638 Training Step 170 \"loss\" =  1136.8573\n",
      "2018-09-02 22:49:16.599724 Test Step 175 Finished\n",
      "2018-09-02 22:49:16.599827 Test Step 175 \"min loss\" =  1107.5449\n",
      "2018-09-02 22:49:16.599885 Test Step 175 \"loss\" =  1107.5449\n",
      "2018-09-02 22:49:19.348585 Training Step 175 Finished Timing (Training: 0.919142, Test: 0.0808251) after 15.1069 seconds\n",
      "2018-09-02 22:49:19.348862 Training Step 175 \"min loss\" =  1099.172\n",
      "2018-09-02 22:49:19.348996 Training Step 175 \"loss\" =  1099.172\n",
      "2018-09-02 22:49:31.711685 Test Step 180 Finished\n",
      "2018-09-02 22:49:31.711785 Test Step 180 \"min loss\" =  1069.5605\n",
      "2018-09-02 22:49:31.711830 Test Step 180 \"loss\" =  1069.5605\n",
      "2018-09-02 22:49:34.423522 Training Step 180 Finished Timing (Training: 0.919153, Test: 0.080813) after 15.0745 seconds\n",
      "2018-09-02 22:49:34.423620 Training Step 180 \"min loss\" =  1063.2288\n",
      "2018-09-02 22:49:34.423672 Training Step 180 \"loss\" =  1063.2288\n",
      "2018-09-02 22:49:47.709431 Test Step 185 Finished\n",
      "2018-09-02 22:49:47.709788 Test Step 185 \"min loss\" =  1035.9618\n",
      "2018-09-02 22:49:47.709919 Test Step 185 \"loss\" =  1035.9618\n",
      "2018-09-02 22:49:50.693652 Training Step 185 Finished Timing (Training: 0.919271, Test: 0.0806935) after 16.2699 seconds\n",
      "2018-09-02 22:49:50.693741 Training Step 185 \"min loss\" =  1019.6903\n",
      "2018-09-02 22:49:50.693784 Training Step 185 \"loss\" =  1019.6903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-09-02 22:50:03.006219 Test Step 190 Finished\n",
      "2018-09-02 22:50:03.006324 Test Step 190 \"min loss\" =  1010.6406\n",
      "2018-09-02 22:50:03.006673 Test Step 190 \"loss\" =  1010.6406\n",
      "2018-09-02 22:50:06.104165 Training Step 190 Finished Timing (Training: 0.919074, Test: 0.0808906) after 15.4103 seconds\n",
      "2018-09-02 22:50:06.104265 Training Step 190 \"min loss\" =  986.927\n",
      "2018-09-02 22:50:06.104314 Training Step 190 \"loss\" =  986.927\n",
      "2018-09-02 22:50:18.912268 Test Step 195 Finished\n",
      "2018-09-02 22:50:18.912364 Test Step 195 \"min loss\" =  964.19586\n",
      "2018-09-02 22:50:18.912413 Test Step 195 \"loss\" =  964.19586\n",
      "2018-09-02 22:50:22.004059 Training Step 195 Finished Timing (Training: 0.919046, Test: 0.080919) after 15.8997 seconds\n",
      "2018-09-02 22:50:22.004165 Training Step 195 \"min loss\" =  948.68335\n",
      "2018-09-02 22:50:22.004217 Training Step 195 \"loss\" =  948.68335\n",
      "2018-09-02 22:50:34.386978 Test Step 200 Finished\n",
      "2018-09-02 22:50:34.387079 Test Step 200 \"min loss\" =  926.84247\n",
      "2018-09-02 22:50:34.387126 Test Step 200 \"loss\" =  926.84247\n",
      "2018-09-02 22:50:37.434008 Training Step 200 Finished Timing (Training: 0.919236, Test: 0.0807288) after 15.4297 seconds\n",
      "2018-09-02 22:50:37.434121 Training Step 200 \"min loss\" =  913.12585\n",
      "2018-09-02 22:50:37.434170 Training Step 200 \"loss\" =  913.12585\n",
      "2018-09-02 22:50:49.944797 Test Step 205 Finished\n",
      "2018-09-02 22:50:49.944911 Test Step 205 \"min loss\" =  905.1458\n",
      "2018-09-02 22:50:49.945247 Test Step 205 \"loss\" =  905.1458\n",
      "2018-09-02 22:50:52.819526 Training Step 205 Finished Timing (Training: 0.921913, Test: 0.0780525) after 15.3853 seconds\n",
      "2018-09-02 22:50:52.819631 Training Step 205 \"min loss\" =  878.21967\n",
      "2018-09-02 22:50:52.819684 Training Step 205 \"loss\" =  878.21967\n",
      "2018-09-02 22:51:05.063333 Test Step 210 Finished\n",
      "2018-09-02 22:51:05.063433 Test Step 210 \"min loss\" =  856.9821\n",
      "2018-09-02 22:51:05.063481 Test Step 210 \"loss\" =  856.9821\n",
      "2018-09-02 22:51:07.848248 Training Step 210 Finished Timing (Training: 0.922324, Test: 0.0776449) after 15.0285 seconds\n",
      "2018-09-02 22:51:07.848357 Training Step 210 \"min loss\" =  842.0912\n",
      "2018-09-02 22:51:07.848410 Training Step 210 \"loss\" =  842.0912\n",
      "2018-09-02 22:51:20.367578 Test Step 215 Finished\n",
      "2018-09-02 22:51:20.367694 Test Step 215 \"min loss\" =  817.30273\n",
      "2018-09-02 22:51:20.367751 Test Step 215 \"loss\" =  817.30273\n",
      "2018-09-02 22:51:23.119456 Training Step 215 Finished Timing (Training: 0.922697, Test: 0.0772717) after 15.271 seconds\n",
      "2018-09-02 22:51:23.119544 Training Step 215 \"min loss\" =  804.53485\n",
      "2018-09-02 22:51:23.119584 Training Step 215 \"loss\" =  804.53485\n",
      "2018-09-02 22:51:35.775671 Test Step 220 Finished\n",
      "2018-09-02 22:51:35.776089 Test Step 220 \"min loss\" =  798.92194\n",
      "2018-09-02 22:51:35.776198 Test Step 220 \"loss\" =  798.92194\n",
      "2018-09-02 22:51:38.665819 Training Step 220 Finished Timing (Training: 0.921338, Test: 0.0786257) after 15.5462 seconds\n",
      "2018-09-02 22:51:38.665920 Training Step 220 \"min loss\" =  769.9921\n",
      "2018-09-02 22:51:38.665967 Training Step 220 \"loss\" =  769.9921\n",
      "2018-09-02 22:51:50.598277 Test Step 225 Finished\n",
      "2018-09-02 22:51:50.598616 Test Step 225 \"min loss\" =  775.73816\n",
      "2018-09-02 22:51:50.598681 Test Step 225 \"loss\" =  775.73816\n",
      "2018-09-02 22:51:53.488353 Training Step 225 Finished Timing (Training: 0.921667, Test: 0.0782952) after 14.8223 seconds\n",
      "2018-09-02 22:51:53.488449 Training Step 225 \"min loss\" =  738.8912\n",
      "2018-09-02 22:51:53.488497 Training Step 225 \"loss\" =  738.8912\n",
      "2018-09-02 22:52:05.529967 Test Step 230 Finished\n",
      "2018-09-02 22:52:05.530113 Test Step 230 \"min loss\" =  741.991\n",
      "2018-09-02 22:52:05.530187 Test Step 230 \"loss\" =  741.991\n",
      "2018-09-02 22:52:08.319468 Training Step 230 Finished Timing (Training: 0.921764, Test: 0.0781984) after 14.8309 seconds\n",
      "2018-09-02 22:52:08.319574 Training Step 230 \"min loss\" =  710.5835\n",
      "2018-09-02 22:52:08.319627 Training Step 230 \"loss\" =  710.5835\n",
      "2018-09-02 22:52:20.580319 Test Step 235 Finished\n",
      "2018-09-02 22:52:20.580407 Test Step 235 \"min loss\" =  731.6669\n",
      "2018-09-02 22:52:20.580444 Test Step 235 \"loss\" =  731.6669\n",
      "2018-09-02 22:52:23.481842 Training Step 235 Finished Timing (Training: 0.921878, Test: 0.0780864) after 15.1622 seconds\n",
      "2018-09-02 22:52:23.481938 Training Step 235 \"min loss\" =  675.50446\n",
      "2018-09-02 22:52:23.481986 Training Step 235 \"loss\" =  675.50446\n",
      "2018-09-02 22:52:37.087905 Test Step 240 Finished\n",
      "2018-09-02 22:52:37.088007 Test Step 240 \"min loss\" =  683.6587\n",
      "2018-09-02 22:52:37.088051 Test Step 240 \"loss\" =  683.6587\n",
      "2018-09-02 22:52:40.001867 Training Step 240 Finished Timing (Training: 0.922944, Test: 0.0770215) after 16.5198 seconds\n",
      "2018-09-02 22:52:40.001996 Training Step 240 \"min loss\" =  647.1125\n",
      "2018-09-02 22:52:40.002072 Training Step 240 \"loss\" =  647.1125\n",
      "2018-09-02 22:52:52.628625 Test Step 245 Finished\n",
      "2018-09-02 22:52:52.628713 Test Step 245 \"min loss\" =  664.55524\n",
      "2018-09-02 22:52:52.628751 Test Step 245 \"loss\" =  664.55524\n",
      "2018-09-02 22:52:55.290682 Training Step 245 Finished Timing (Training: 0.923106, Test: 0.0768602) after 15.2885 seconds\n",
      "2018-09-02 22:52:55.290788 Training Step 245 \"min loss\" =  617.10486\n",
      "2018-09-02 22:52:55.290840 Training Step 245 \"loss\" =  617.10486\n",
      "2018-09-02 22:53:07.564837 Test Step 250 Finished\n",
      "2018-09-02 22:53:07.564926 Test Step 250 \"min loss\" =  636.1376\n",
      "2018-09-02 22:53:07.564966 Test Step 250 \"loss\" =  636.1376\n",
      "2018-09-02 22:53:10.163822 Training Step 250 Finished Timing (Training: 0.923003, Test: 0.0769636) after 14.8729 seconds\n",
      "2018-09-02 22:53:10.163922 Training Step 250 \"min loss\" =  585.3309\n",
      "2018-09-02 22:53:10.163970 Training Step 250 \"loss\" =  585.3309\n",
      "2018-09-02 22:53:22.760830 Test Step 255 Finished\n",
      "2018-09-02 22:53:22.761091 Test Step 255 \"min loss\" =  603.0359\n",
      "2018-09-02 22:53:22.761134 Test Step 255 \"loss\" =  603.0359\n",
      "2018-09-02 22:53:25.408030 Training Step 255 Finished Timing (Training: 0.921907, Test: 0.0780589) after 15.244 seconds\n",
      "2018-09-02 22:53:25.408136 Training Step 255 \"min loss\" =  558.00574\n",
      "2018-09-02 22:53:25.408194 Training Step 255 \"loss\" =  558.00574\n",
      "2018-09-02 22:53:37.773861 Test Step 260 Finished\n",
      "2018-09-02 22:53:37.774236 Test Step 260 \"min loss\" =  573.11664\n",
      "2018-09-02 22:53:37.774290 Test Step 260 \"loss\" =  573.11664\n",
      "2018-09-02 22:53:40.825479 Training Step 260 Finished Timing (Training: 0.920427, Test: 0.0795384) after 15.4172 seconds\n",
      "2018-09-02 22:53:40.825575 Training Step 260 \"min loss\" =  530.03\n",
      "2018-09-02 22:53:40.825623 Training Step 260 \"loss\" =  530.03\n",
      "2018-09-02 22:53:53.463646 Test Step 265 Finished\n",
      "2018-09-02 22:53:53.463914 Test Step 265 \"min loss\" =  563.1589\n",
      "2018-09-02 22:53:53.463955 Test Step 265 \"loss\" =  563.1589\n",
      "2018-09-02 22:53:56.377262 Training Step 265 Finished Timing (Training: 0.920509, Test: 0.0794559) after 15.5516 seconds\n",
      "2018-09-02 22:53:56.377360 Training Step 265 \"min loss\" =  504.24283\n",
      "2018-09-02 22:53:56.377408 Training Step 265 \"loss\" =  504.24283\n",
      "2018-09-02 22:54:09.060599 Test Step 270 Finished\n",
      "2018-09-02 22:54:09.060688 Test Step 270 \"min loss\" =  553.7453\n",
      "2018-09-02 22:54:09.061284 Test Step 270 \"loss\" =  553.7453\n",
      "2018-09-02 22:54:11.860100 Training Step 270 Finished Timing (Training: 0.920663, Test: 0.0792997) after 15.4826 seconds\n",
      "2018-09-02 22:54:11.860208 Training Step 270 \"min loss\" =  478.88614\n",
      "2018-09-02 22:54:11.860261 Training Step 270 \"loss\" =  478.88614\n",
      "2018-09-02 22:54:23.730857 Test Step 275 Finished\n",
      "2018-09-02 22:54:23.730947 Test Step 275 \"min loss\" =  521.0484\n",
      "2018-09-02 22:54:23.730991 Test Step 275 \"loss\" =  521.0484\n",
      "2018-09-02 22:54:26.375500 Training Step 275 Finished Timing (Training: 0.920727, Test: 0.0792363) after 14.5152 seconds\n",
      "2018-09-02 22:54:26.375594 Training Step 275 \"min loss\" =  453.7527\n",
      "2018-09-02 22:54:26.375635 Training Step 275 \"loss\" =  453.7527\n",
      "2018-09-02 22:54:39.543779 Test Step 280 Finished\n",
      "2018-09-02 22:54:39.543891 Test Step 280 \"min loss\" =  495.67612\n",
      "2018-09-02 22:54:39.543945 Test Step 280 \"loss\" =  495.67612\n",
      "2018-09-02 22:54:42.510927 Training Step 280 Finished Timing (Training: 0.920516, Test: 0.0794476) after 16.1352 seconds\n",
      "2018-09-02 22:54:42.511023 Training Step 280 \"min loss\" =  429.82788\n",
      "2018-09-02 22:54:42.511066 Training Step 280 \"loss\" =  429.82788\n",
      "2018-09-02 22:54:55.006164 Test Step 285 Finished\n",
      "2018-09-02 22:54:55.006428 Test Step 285 \"min loss\" =  495.67612\n",
      "2018-09-02 22:54:55.006470 Test Step 285 \"loss\" =  496.2818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-09-02 22:54:57.931382 Training Step 285 Finished Timing (Training: 0.920725, Test: 0.0792387) after 15.4203 seconds\n",
      "2018-09-02 22:54:57.931482 Training Step 285 \"min loss\" =  404.51752\n",
      "2018-09-02 22:54:57.931529 Training Step 285 \"loss\" =  404.51752\n",
      "2018-09-02 22:55:10.469210 Test Step 290 Finished\n",
      "2018-09-02 22:55:10.469301 Test Step 290 \"min loss\" =  449.603\n",
      "2018-09-02 22:55:10.469340 Test Step 290 \"loss\" =  449.603\n",
      "2018-09-02 22:55:13.336440 Training Step 290 Finished Timing (Training: 0.92097, Test: 0.0789944) after 15.4049 seconds\n",
      "2018-09-02 22:55:13.336563 Training Step 290 \"min loss\" =  384.43152\n",
      "2018-09-02 22:55:13.336631 Training Step 290 \"loss\" =  384.43152\n",
      "2018-09-02 22:55:24.949121 Test Step 295 Finished\n",
      "2018-09-02 22:55:24.949234 Test Step 295 \"min loss\" =  418.16928\n",
      "2018-09-02 22:55:24.949294 Test Step 295 \"loss\" =  418.16928\n",
      "2018-09-02 22:55:27.858229 Training Step 295 Finished Timing (Training: 0.920953, Test: 0.0790115) after 14.5215 seconds\n",
      "2018-09-02 22:55:27.858336 Training Step 295 \"min loss\" =  363.3725\n",
      "2018-09-02 22:55:27.858390 Training Step 295 \"loss\" =  363.3725\n",
      "2018-09-02 22:55:41.436445 Test Step 300 Finished\n",
      "2018-09-02 22:55:41.436540 Test Step 300 \"min loss\" =  411.13214\n",
      "2018-09-02 22:55:41.436584 Test Step 300 \"loss\" =  411.13214\n",
      "2018-09-02 22:55:44.444288 Training Step 300 Finished Timing (Training: 0.921103, Test: 0.0788618) after 16.5858 seconds\n",
      "2018-09-02 22:55:44.444377 Training Step 300 \"min loss\" =  343.68076\n",
      "2018-09-02 22:55:44.444419 Training Step 300 \"loss\" =  343.68076\n",
      "2018-09-02 22:55:57.399282 Test Step 305 Finished\n",
      "2018-09-02 22:55:57.399384 Test Step 305 \"min loss\" =  390.0587\n",
      "2018-09-02 22:55:57.399428 Test Step 305 \"loss\" =  390.0587\n",
      "2018-09-02 22:56:00.258703 Training Step 305 Finished Timing (Training: 0.924007, Test: 0.07598) after 15.8142 seconds\n",
      "2018-09-02 22:56:00.258807 Training Step 305 \"min loss\" =  322.99054\n",
      "2018-09-02 22:56:00.258858 Training Step 305 \"loss\" =  323.74966\n",
      "2018-09-02 22:56:13.151546 Test Step 310 Finished\n",
      "2018-09-02 22:56:13.151633 Test Step 310 \"min loss\" =  367.43515\n",
      "2018-09-02 22:56:13.151669 Test Step 310 \"loss\" =  367.43515\n",
      "2018-09-02 22:56:15.929557 Training Step 310 Finished Timing (Training: 0.924408, Test: 0.0755719) after 15.6706 seconds\n",
      "2018-09-02 22:56:15.929663 Training Step 310 \"min loss\" =  303.94382\n",
      "2018-09-02 22:56:15.929742 Training Step 310 \"loss\" =  303.94382\n",
      "2018-09-02 22:56:28.304196 Test Step 315 Finished\n",
      "2018-09-02 22:56:28.304294 Test Step 315 \"min loss\" =  342.67502\n",
      "2018-09-02 22:56:28.304836 Test Step 315 \"loss\" =  342.67502\n",
      "2018-09-02 22:56:30.904706 Training Step 315 Finished Timing (Training: 0.92424, Test: 0.0757261) after 14.9749 seconds\n",
      "2018-09-02 22:56:30.904804 Training Step 315 \"min loss\" =  289.71216\n",
      "2018-09-02 22:56:30.904851 Training Step 315 \"loss\" =  289.74283\n",
      "2018-09-02 22:56:43.261384 Test Step 320 Finished\n",
      "2018-09-02 22:56:43.261478 Test Step 320 \"min loss\" =  337.32367\n",
      "2018-09-02 22:56:43.261525 Test Step 320 \"loss\" =  337.32367\n",
      "2018-09-02 22:56:45.959882 Training Step 320 Finished Timing (Training: 0.922525, Test: 0.0774428) after 15.055 seconds\n",
      "2018-09-02 22:56:45.959973 Training Step 320 \"min loss\" =  267.66016\n",
      "2018-09-02 22:56:45.960016 Training Step 320 \"loss\" =  267.66016\n",
      "2018-09-02 22:56:58.433817 Test Step 325 Finished\n",
      "2018-09-02 22:56:58.433913 Test Step 325 \"min loss\" =  314.70404\n",
      "2018-09-02 22:56:58.433956 Test Step 325 \"loss\" =  314.70404\n",
      "2018-09-02 22:57:01.103510 Training Step 325 Finished Timing (Training: 0.92083, Test: 0.0791393) after 15.1434 seconds\n",
      "2018-09-02 22:57:01.103610 Training Step 325 \"min loss\" =  254.24129\n",
      "2018-09-02 22:57:01.103658 Training Step 325 \"loss\" =  254.24129\n",
      "2018-09-02 22:57:13.341867 Test Step 330 Finished\n",
      "2018-09-02 22:57:13.341992 Test Step 330 \"min loss\" =  305.21765\n",
      "2018-09-02 22:57:13.342056 Test Step 330 \"loss\" =  305.21765\n",
      "2018-09-02 22:57:16.155805 Training Step 330 Finished Timing (Training: 0.919875, Test: 0.0800941) after 15.0521 seconds\n",
      "2018-09-02 22:57:16.155899 Training Step 330 \"min loss\" =  240.56523\n",
      "2018-09-02 22:57:16.155942 Training Step 330 \"loss\" =  242.07155\n",
      "2018-09-02 22:57:28.390425 Test Step 335 Finished\n",
      "2018-09-02 22:57:28.390531 Test Step 335 \"min loss\" =  300.85834\n",
      "2018-09-02 22:57:28.390790 Test Step 335 \"loss\" =  300.85834\n",
      "2018-09-02 22:57:31.229597 Training Step 335 Finished Timing (Training: 0.918928, Test: 0.0810396) after 15.0736 seconds\n",
      "2018-09-02 22:57:31.229685 Training Step 335 \"min loss\" =  227.01707\n",
      "2018-09-02 22:57:31.229726 Training Step 335 \"loss\" =  227.01707\n",
      "2018-09-02 22:57:43.251591 Test Step 340 Finished\n",
      "2018-09-02 22:57:43.251695 Test Step 340 \"min loss\" =  287.55545\n",
      "2018-09-02 22:57:43.251741 Test Step 340 \"loss\" =  287.55545\n",
      "2018-09-02 22:57:46.128808 Training Step 340 Finished Timing (Training: 0.919212, Test: 0.0807564) after 14.899 seconds\n",
      "2018-09-02 22:57:46.128904 Training Step 340 \"min loss\" =  209.83629\n",
      "2018-09-02 22:57:46.128951 Training Step 340 \"loss\" =  209.83629\n",
      "2018-09-02 22:57:58.717446 Test Step 345 Finished\n",
      "2018-09-02 22:57:58.717555 Test Step 345 \"min loss\" =  270.01477\n",
      "2018-09-02 22:57:58.717601 Test Step 345 \"loss\" =  270.01477\n",
      "2018-09-02 22:58:01.651296 Training Step 345 Finished Timing (Training: 0.91997, Test: 0.0799987) after 15.5223 seconds\n",
      "2018-09-02 22:58:01.651398 Training Step 345 \"min loss\" =  197.48799\n",
      "2018-09-02 22:58:01.651445 Training Step 345 \"loss\" =  197.48799\n",
      "2018-09-02 22:58:14.361465 Test Step 350 Finished\n",
      "2018-09-02 22:58:14.361602 Test Step 350 \"min loss\" =  252.28842\n",
      "2018-09-02 22:58:14.361660 Test Step 350 \"loss\" =  252.28842\n",
      "2018-09-02 22:58:17.300290 Training Step 350 Finished Timing (Training: 0.920053, Test: 0.0799142) after 15.6488 seconds\n",
      "2018-09-02 22:58:17.300401 Training Step 350 \"min loss\" =  186.68475\n",
      "2018-09-02 22:58:17.300464 Training Step 350 \"loss\" =  186.95555\n",
      "2018-09-02 22:58:30.905765 Test Step 355 Finished\n",
      "2018-09-02 22:58:30.905884 Test Step 355 \"min loss\" =  242.8038\n",
      "2018-09-02 22:58:30.905952 Test Step 355 \"loss\" =  242.8038\n",
      "2018-09-02 22:58:35.290200 Training Step 355 Finished Timing (Training: 0.919811, Test: 0.0801563) after 17.9897 seconds\n",
      "2018-09-02 22:58:35.290303 Training Step 355 \"min loss\" =  178.73294\n",
      "2018-09-02 22:58:35.290353 Training Step 355 \"loss\" =  178.73294\n",
      "2018-09-02 22:58:52.599355 Test Step 360 Finished\n",
      "2018-09-02 22:58:52.599479 Test Step 360 \"min loss\" =  231.49132\n",
      "2018-09-02 22:58:52.599546 Test Step 360 \"loss\" =  231.49132\n",
      "2018-09-02 22:58:56.420284 Training Step 360 Finished Timing (Training: 0.918845, Test: 0.0811238) after 21.1299 seconds\n",
      "2018-09-02 22:58:56.420418 Training Step 360 \"min loss\" =  166.00296\n",
      "2018-09-02 22:58:56.420488 Training Step 360 \"loss\" =  168.18077\n",
      "2018-09-02 22:59:12.684773 Test Step 365 Finished\n",
      "2018-09-02 22:59:12.684881 Test Step 365 \"min loss\" =  225.18651\n",
      "2018-09-02 22:59:12.684924 Test Step 365 \"loss\" =  225.18651\n",
      "2018-09-02 22:59:16.159686 Training Step 365 Finished Timing (Training: 0.919009, Test: 0.0809591) after 19.7391 seconds\n",
      "2018-09-02 22:59:16.159838 Training Step 365 \"min loss\" =  156.90958\n",
      "2018-09-02 22:59:16.159934 Training Step 365 \"loss\" =  156.90958\n",
      "2018-09-02 22:59:31.543884 Test Step 370 Finished\n",
      "2018-09-02 22:59:31.544041 Test Step 370 \"min loss\" =  219.8674\n",
      "2018-09-02 22:59:31.544300 Test Step 370 \"loss\" =  219.8674\n",
      "2018-09-02 22:59:35.230999 Training Step 370 Finished Timing (Training: 0.918944, Test: 0.0810232) after 19.071 seconds\n",
      "2018-09-02 22:59:35.231086 Training Step 370 \"min loss\" =  147.0643\n",
      "2018-09-02 22:59:35.231129 Training Step 370 \"loss\" =  147.0643\n",
      "2018-09-02 22:59:50.617108 Test Step 375 Finished\n",
      "2018-09-02 22:59:50.617199 Test Step 375 \"min loss\" =  211.41054\n",
      "2018-09-02 22:59:50.617242 Test Step 375 \"loss\" =  211.41054\n",
      "2018-09-02 22:59:53.287691 Training Step 375 Finished Timing (Training: 0.919666, Test: 0.0803003) after 18.0565 seconds\n",
      "2018-09-02 22:59:53.287776 Training Step 375 \"min loss\" =  137.62012\n",
      "2018-09-02 22:59:53.287818 Training Step 375 \"loss\" =  137.62012\n",
      "2018-09-02 23:00:06.384991 Test Step 380 Finished\n",
      "2018-09-02 23:00:06.385096 Test Step 380 \"min loss\" =  197.77545\n",
      "2018-09-02 23:00:06.385142 Test Step 380 \"loss\" =  197.77545\n",
      "2018-09-02 23:00:09.152034 Training Step 380 Finished Timing (Training: 0.919316, Test: 0.0806507) after 15.8642 seconds\n",
      "2018-09-02 23:00:09.152147 Training Step 380 \"min loss\" =  128.74513\n",
      "2018-09-02 23:00:09.152207 Training Step 380 \"loss\" =  128.74513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-09-02 23:00:21.314653 Test Step 385 Finished\n",
      "2018-09-02 23:00:21.314751 Test Step 385 \"min loss\" =  187.73625\n",
      "2018-09-02 23:00:21.314800 Test Step 385 \"loss\" =  187.73625\n",
      "2018-09-02 23:00:23.866914 Training Step 385 Finished Timing (Training: 0.919478, Test: 0.0804894) after 14.7146 seconds\n",
      "2018-09-02 23:00:23.867016 Training Step 385 \"min loss\" =  121.51582\n",
      "2018-09-02 23:00:23.867070 Training Step 385 \"loss\" =  121.51582\n",
      "2018-09-02 23:00:36.844279 Test Step 390 Finished\n",
      "2018-09-02 23:00:36.844372 Test Step 390 \"min loss\" =  178.87193\n",
      "2018-09-02 23:00:36.844414 Test Step 390 \"loss\" =  178.87193\n",
      "2018-09-02 23:00:39.474530 Training Step 390 Finished Timing (Training: 0.919888, Test: 0.0800798) after 15.6074 seconds\n",
      "2018-09-02 23:00:39.474637 Training Step 390 \"min loss\" =  113.32627\n",
      "2018-09-02 23:00:39.474688 Training Step 390 \"loss\" =  113.32627\n",
      "2018-09-02 23:00:52.145998 Test Step 395 Finished\n",
      "2018-09-02 23:00:52.146104 Test Step 395 \"min loss\" =  173.8463\n",
      "2018-09-02 23:00:52.146153 Test Step 395 \"loss\" =  173.8463\n",
      "2018-09-02 23:00:54.766520 Training Step 395 Finished Timing (Training: 0.920128, Test: 0.07984) after 15.2918 seconds\n",
      "2018-09-02 23:00:54.766816 Training Step 395 \"min loss\" =  106.50543\n",
      "2018-09-02 23:00:54.766895 Training Step 395 \"loss\" =  106.50543\n",
      "2018-09-02 23:01:07.562712 Test Step 400 Finished\n",
      "2018-09-02 23:01:07.562800 Test Step 400 \"min loss\" =  173.8463\n",
      "2018-09-02 23:01:07.562852 Test Step 400 \"loss\" =  174.89032\n",
      "2018-09-02 23:01:10.346740 Training Step 400 Finished Timing (Training: 0.920154, Test: 0.079813) after 15.5798 seconds\n",
      "2018-09-02 23:01:10.347029 Training Step 400 \"min loss\" =  102.70686\n",
      "2018-09-02 23:01:10.347123 Training Step 400 \"loss\" =  102.70686\n",
      "2018-09-02 23:01:23.058401 Test Step 405 Finished\n",
      "2018-09-02 23:01:23.058532 Test Step 405 \"min loss\" =  169.63934\n",
      "2018-09-02 23:01:23.058615 Test Step 405 \"loss\" =  169.63934\n",
      "2018-09-02 23:01:25.755618 Training Step 405 Finished Timing (Training: 0.912638, Test: 0.0873413) after 15.4084 seconds\n",
      "2018-09-02 23:01:25.755706 Training Step 405 \"min loss\" =  96.31418\n",
      "2018-09-02 23:01:25.755748 Training Step 405 \"loss\" =  99.468475\n",
      "2018-09-02 23:01:38.736002 Test Step 410 Finished\n",
      "2018-09-02 23:01:38.736100 Test Step 410 \"min loss\" =  160.74104\n",
      "2018-09-02 23:01:38.736344 Test Step 410 \"loss\" =  160.74104\n",
      "2018-09-02 23:01:41.430790 Training Step 410 Finished Timing (Training: 0.913887, Test: 0.0860837) after 15.675 seconds\n",
      "2018-09-02 23:01:41.430882 Training Step 410 \"min loss\" =  92.59155\n",
      "2018-09-02 23:01:41.430925 Training Step 410 \"loss\" =  92.59155\n",
      "2018-09-02 23:01:54.234447 Test Step 415 Finished\n",
      "2018-09-02 23:01:54.234543 Test Step 415 \"min loss\" =  157.77063\n",
      "2018-09-02 23:01:54.234585 Test Step 415 \"loss\" =  157.77063\n",
      "2018-09-02 23:01:57.029290 Training Step 415 Finished Timing (Training: 0.91425, Test: 0.0857223) after 15.5983 seconds\n",
      "2018-09-02 23:01:57.029380 Training Step 415 \"min loss\" =  88.21034\n",
      "2018-09-02 23:01:57.029422 Training Step 415 \"loss\" =  90.84495\n",
      "2018-09-02 23:02:09.161171 Test Step 420 Finished\n",
      "2018-09-02 23:02:09.161277 Test Step 420 \"min loss\" =  157.77063\n",
      "2018-09-02 23:02:09.161327 Test Step 420 \"loss\" =  160.22418\n",
      "2018-09-02 23:02:11.956143 Training Step 420 Finished Timing (Training: 0.914463, Test: 0.0855089) after 14.9267 seconds\n",
      "2018-09-02 23:02:11.956233 Training Step 420 \"min loss\" =  84.94427\n",
      "2018-09-02 23:02:11.956274 Training Step 420 \"loss\" =  84.94427\n",
      "2018-09-02 23:02:24.333431 Test Step 425 Finished\n",
      "2018-09-02 23:02:24.333523 Test Step 425 \"min loss\" =  157.46538\n",
      "2018-09-02 23:02:24.333570 Test Step 425 \"loss\" =  157.46538\n",
      "2018-09-02 23:02:27.249823 Training Step 425 Finished Timing (Training: 0.914347, Test: 0.085626) after 15.2935 seconds\n",
      "2018-09-02 23:02:27.249911 Training Step 425 \"min loss\" =  80.44728\n",
      "2018-09-02 23:02:27.249950 Training Step 425 \"loss\" =  83.651924\n",
      "2018-09-02 23:02:39.124813 Test Step 430 Finished\n",
      "2018-09-02 23:02:39.124938 Test Step 430 \"min loss\" =  146.99556\n",
      "2018-09-02 23:02:39.124999 Test Step 430 \"loss\" =  146.99556\n",
      "2018-09-02 23:02:42.433656 Training Step 430 Finished Timing (Training: 0.91562, Test: 0.08435) after 15.1834 seconds\n",
      "2018-09-02 23:02:42.433795 Training Step 430 \"min loss\" =  78.27267\n",
      "2018-09-02 23:02:42.433882 Training Step 430 \"loss\" =  78.27267\n",
      "2018-09-02 23:02:55.069942 Test Step 435 Finished\n",
      "2018-09-02 23:02:55.070044 Test Step 435 \"min loss\" =  140.9269\n",
      "2018-09-02 23:02:55.070391 Test Step 435 \"loss\" =  140.9269\n",
      "2018-09-02 23:02:57.845780 Training Step 435 Finished Timing (Training: 0.915805, Test: 0.0841612) after 15.4118 seconds\n",
      "2018-09-02 23:02:57.845872 Training Step 435 \"min loss\" =  75.43453\n",
      "2018-09-02 23:02:57.845914 Training Step 435 \"loss\" =  75.43453\n",
      "2018-09-02 23:03:10.626770 Test Step 440 Finished\n",
      "2018-09-02 23:03:10.626872 Test Step 440 \"min loss\" =  139.63542\n",
      "2018-09-02 23:03:10.626930 Test Step 440 \"loss\" =  139.63542\n",
      "2018-09-02 23:03:13.370056 Training Step 440 Finished Timing (Training: 0.915555, Test: 0.0844125) after 15.5241 seconds\n",
      "2018-09-02 23:03:13.370155 Training Step 440 \"min loss\" =  74.133224\n",
      "2018-09-02 23:03:13.370197 Training Step 440 \"loss\" =  74.133224\n",
      "2018-09-02 23:03:26.228101 Test Step 445 Finished\n",
      "2018-09-02 23:03:26.228217 Test Step 445 \"min loss\" =  139.19376\n",
      "2018-09-02 23:03:26.228282 Test Step 445 \"loss\" =  139.19376\n",
      "2018-09-02 23:03:29.228614 Training Step 445 Finished Timing (Training: 0.915654, Test: 0.0843135) after 15.8584 seconds\n",
      "2018-09-02 23:03:29.228742 Training Step 445 \"min loss\" =  68.052795\n",
      "2018-09-02 23:03:29.228817 Training Step 445 \"loss\" =  71.20571\n",
      "2018-09-02 23:03:42.461135 Test Step 450 Finished\n",
      "2018-09-02 23:03:42.461238 Test Step 450 \"min loss\" =  139.19376\n",
      "2018-09-02 23:03:42.461289 Test Step 450 \"loss\" =  143.03516\n",
      "2018-09-02 23:03:45.568363 Training Step 450 Finished Timing (Training: 0.916048, Test: 0.0839197) after 16.3395 seconds\n",
      "2018-09-02 23:03:45.568463 Training Step 450 \"min loss\" =  66.36042\n",
      "2018-09-02 23:03:45.568513 Training Step 450 \"loss\" =  69.016754\n",
      "2018-09-02 23:03:57.620854 Test Step 455 Finished\n",
      "2018-09-02 23:03:57.620937 Test Step 455 \"min loss\" =  132.99191\n",
      "2018-09-02 23:03:57.621180 Test Step 455 \"loss\" =  132.99191\n",
      "2018-09-02 23:04:00.205461 Training Step 455 Finished Timing (Training: 0.916642, Test: 0.0833247) after 14.6369 seconds\n",
      "2018-09-02 23:04:00.205547 Training Step 455 \"min loss\" =  63.503227\n",
      "2018-09-02 23:04:00.205585 Training Step 455 \"loss\" =  63.503227\n",
      "2018-09-02 23:04:13.739325 Test Step 460 Finished\n",
      "2018-09-02 23:04:13.739428 Test Step 460 \"min loss\" =  127.59957\n",
      "2018-09-02 23:04:13.739477 Test Step 460 \"loss\" =  127.59957\n",
      "2018-09-02 23:04:16.758433 Training Step 460 Finished Timing (Training: 0.917754, Test: 0.0822122) after 16.5526 seconds\n",
      "2018-09-02 23:04:16.758564 Training Step 460 \"min loss\" =  63.17007\n",
      "2018-09-02 23:04:16.758632 Training Step 460 \"loss\" =  63.17007\n",
      "2018-09-02 23:04:29.563156 Test Step 465 Finished\n",
      "2018-09-02 23:04:29.563256 Test Step 465 \"min loss\" =  127.59957\n",
      "2018-09-02 23:04:29.563490 Test Step 465 \"loss\" =  130.87952\n",
      "2018-09-02 23:04:32.274923 Training Step 465 Finished Timing (Training: 0.918404, Test: 0.0815613) after 15.5162 seconds\n",
      "2018-09-02 23:04:32.275026 Training Step 465 \"min loss\" =  63.11644\n",
      "2018-09-02 23:04:32.275076 Training Step 465 \"loss\" =  63.623837\n",
      "2018-09-02 23:04:44.528350 Test Step 470 Finished\n",
      "2018-09-02 23:04:44.528454 Test Step 470 \"min loss\" =  127.59957\n",
      "2018-09-02 23:04:44.528511 Test Step 470 \"loss\" =  130.03633\n",
      "2018-09-02 23:04:47.184823 Training Step 470 Finished Timing (Training: 0.918708, Test: 0.0812575) after 14.9097 seconds\n",
      "2018-09-02 23:04:47.184912 Training Step 470 \"min loss\" =  59.458782\n",
      "2018-09-02 23:04:47.184953 Training Step 470 \"loss\" =  60.304256\n",
      "2018-09-02 23:04:59.754248 Test Step 475 Finished\n",
      "2018-09-02 23:04:59.754334 Test Step 475 \"min loss\" =  127.59957\n",
      "2018-09-02 23:04:59.754370 Test Step 475 \"loss\" =  127.955124\n",
      "2018-09-02 23:05:02.377153 Training Step 475 Finished Timing (Training: 0.918683, Test: 0.0812831) after 15.1922 seconds\n",
      "2018-09-02 23:05:02.377243 Training Step 475 \"min loss\" =  59.13315\n",
      "2018-09-02 23:05:02.377286 Training Step 475 \"loss\" =  60.20149\n",
      "2018-09-02 23:05:15.249859 Test Step 480 Finished\n",
      "2018-09-02 23:05:15.249963 Test Step 480 \"min loss\" =  127.59957\n",
      "2018-09-02 23:05:15.250009 Test Step 480 \"loss\" =  127.91802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-09-02 23:05:17.953922 Training Step 480 Finished Timing (Training: 0.918446, Test: 0.0815204) after 15.5766 seconds\n",
      "2018-09-02 23:05:17.954021 Training Step 480 \"min loss\" =  55.68774\n",
      "2018-09-02 23:05:17.954067 Training Step 480 \"loss\" =  58.348495\n",
      "2018-09-02 23:05:30.863751 Test Step 485 Finished\n",
      "2018-09-02 23:05:30.863853 Test Step 485 \"min loss\" =  123.83815\n",
      "2018-09-02 23:05:30.863898 Test Step 485 \"loss\" =  123.83815\n",
      "2018-09-02 23:05:33.505808 Training Step 485 Finished Timing (Training: 0.918297, Test: 0.0816702) after 15.5517 seconds\n",
      "2018-09-02 23:05:33.505910 Training Step 485 \"min loss\" =  54.51204\n",
      "2018-09-02 23:05:33.505964 Training Step 485 \"loss\" =  55.092983\n",
      "2018-09-02 23:05:45.719166 Test Step 490 Finished\n",
      "2018-09-02 23:05:45.719313 Test Step 490 \"min loss\" =  123.83815\n",
      "2018-09-02 23:05:45.719397 Test Step 490 \"loss\" =  124.00079\n",
      "2018-09-02 23:05:48.462806 Training Step 490 Finished Timing (Training: 0.918052, Test: 0.0819145) after 14.9568 seconds\n",
      "2018-09-02 23:05:48.462895 Training Step 490 \"min loss\" =  52.371414\n",
      "2018-09-02 23:05:48.462937 Training Step 490 \"loss\" =  55.045654\n",
      "2018-09-02 23:06:00.476371 Test Step 495 Finished\n",
      "2018-09-02 23:06:00.476473 Test Step 495 \"min loss\" =  123.83815\n",
      "2018-09-02 23:06:00.476519 Test Step 495 \"loss\" =  124.54936\n",
      "2018-09-02 23:06:03.393017 Training Step 495 Finished Timing (Training: 0.917942, Test: 0.0820248) after 14.93 seconds\n",
      "2018-09-02 23:06:03.393116 Training Step 495 \"min loss\" =  51.470585\n",
      "2018-09-02 23:06:03.393164 Training Step 495 \"loss\" =  55.490536\n",
      "2018-09-02 23:06:15.215262 Test Step 500 Finished\n",
      "2018-09-02 23:06:15.215389 Test Step 500 \"min loss\" =  120.70835\n",
      "2018-09-02 23:06:15.215613 Test Step 500 \"loss\" =  120.70835\n",
      "2018-09-02 23:06:18.504444 Training Step 500 Finished Timing (Training: 0.918109, Test: 0.0818574) after 15.1112 seconds\n",
      "2018-09-02 23:06:18.504553 Training Step 500 \"min loss\" =  50.091812\n",
      "2018-09-02 23:06:18.504617 Training Step 500 \"loss\" =  50.091812\n",
      "2018-09-02 23:06:31.346541 Test Step 505 Finished\n",
      "2018-09-02 23:06:31.346645 Test Step 505 \"min loss\" =  120.70835\n",
      "2018-09-02 23:06:31.346693 Test Step 505 \"loss\" =  122.94424\n",
      "2018-09-02 23:06:34.149031 Training Step 505 Finished Timing (Training: 0.915446, Test: 0.0845403) after 15.6444 seconds\n",
      "2018-09-02 23:06:34.149134 Training Step 505 \"min loss\" =  49.015938\n",
      "2018-09-02 23:06:34.149181 Training Step 505 \"loss\" =  51.632404\n",
      "2018-09-02 23:06:47.474255 Test Step 510 Finished\n",
      "2018-09-02 23:06:47.474359 Test Step 510 \"min loss\" =  120.70835\n",
      "2018-09-02 23:06:47.474408 Test Step 510 \"loss\" =  124.82556\n",
      "2018-09-02 23:06:50.427756 Training Step 510 Finished Timing (Training: 0.909363, Test: 0.090617) after 16.2785 seconds\n",
      "2018-09-02 23:06:50.427852 Training Step 510 \"min loss\" =  48.210262\n",
      "2018-09-02 23:06:50.427899 Training Step 510 \"loss\" =  51.23101\n",
      "2018-09-02 23:07:02.888281 Test Step 515 Finished\n",
      "2018-09-02 23:07:02.888392 Test Step 515 \"min loss\" =  120.70835\n",
      "2018-09-02 23:07:02.888451 Test Step 515 \"loss\" =  123.78943\n",
      "2018-09-02 23:07:05.901530 Training Step 515 Finished Timing (Training: 0.913302, Test: 0.0866749) after 15.4736 seconds\n",
      "2018-09-02 23:07:05.901908 Training Step 515 \"min loss\" =  48.210262\n",
      "2018-09-02 23:07:05.901971 Training Step 515 \"loss\" =  49.381077\n",
      "2018-09-02 23:07:18.671360 Test Step 520 Finished\n",
      "2018-09-02 23:07:18.671469 Test Step 520 \"min loss\" =  120.70835\n",
      "2018-09-02 23:07:18.671834 Test Step 520 \"loss\" =  123.2882\n",
      "2018-09-02 23:07:21.600397 Training Step 520 Finished Timing (Training: 0.916068, Test: 0.0838975) after 15.6984 seconds\n",
      "2018-09-02 23:07:21.600811 Training Step 520 \"min loss\" =  45.1705\n",
      "2018-09-02 23:07:21.600987 Training Step 520 \"loss\" =  46.820354\n",
      "2018-09-02 23:07:34.959016 Test Step 525 Finished\n",
      "2018-09-02 23:07:34.959147 Test Step 525 \"min loss\" =  119.53644\n",
      "2018-09-02 23:07:34.959219 Test Step 525 \"loss\" =  119.53644\n",
      "2018-09-02 23:07:37.982516 Training Step 525 Finished Timing (Training: 0.918316, Test: 0.0816447) after 16.3815 seconds\n",
      "2018-09-02 23:07:37.982615 Training Step 525 \"min loss\" =  45.1705\n",
      "2018-09-02 23:07:37.982663 Training Step 525 \"loss\" =  46.583748\n",
      "2018-09-02 23:07:50.152644 Test Step 530 Finished\n",
      "2018-09-02 23:07:50.152737 Test Step 530 \"min loss\" =  116.6977\n",
      "2018-09-02 23:07:50.152777 Test Step 530 \"loss\" =  116.6977\n",
      "2018-09-02 23:07:53.023204 Training Step 530 Finished Timing (Training: 0.919136, Test: 0.0808263) after 15.0405 seconds\n",
      "2018-09-02 23:07:53.023314 Training Step 530 \"min loss\" =  44.044285\n",
      "2018-09-02 23:07:53.023377 Training Step 530 \"loss\" =  44.044285\n",
      "2018-09-02 23:08:05.463764 Test Step 535 Finished\n",
      "2018-09-02 23:08:05.463858 Test Step 535 \"min loss\" =  116.582504\n",
      "2018-09-02 23:08:05.463895 Test Step 535 \"loss\" =  116.582504\n",
      "2018-09-02 23:08:08.314900 Training Step 535 Finished Timing (Training: 0.919862, Test: 0.0801011) after 15.2914 seconds\n",
      "2018-09-02 23:08:08.315017 Training Step 535 \"min loss\" =  44.044285\n",
      "2018-09-02 23:08:08.315081 Training Step 535 \"loss\" =  47.528275\n",
      "2018-09-02 23:08:20.441537 Test Step 540 Finished\n",
      "2018-09-02 23:08:20.441641 Test Step 540 \"min loss\" =  116.582504\n",
      "2018-09-02 23:08:20.441697 Test Step 540 \"loss\" =  119.400665\n",
      "2018-09-02 23:08:23.115265 Training Step 540 Finished Timing (Training: 0.920015, Test: 0.0799484) after 14.8001 seconds\n",
      "2018-09-02 23:08:23.115366 Training Step 540 \"min loss\" =  44.044285\n",
      "2018-09-02 23:08:23.115419 Training Step 540 \"loss\" =  45.43879\n",
      "2018-09-02 23:08:36.200158 Test Step 545 Finished\n",
      "2018-09-02 23:08:36.200589 Test Step 545 \"min loss\" =  116.582504\n",
      "2018-09-02 23:08:36.200635 Test Step 545 \"loss\" =  118.949295\n",
      "2018-09-02 23:08:39.178387 Training Step 545 Finished Timing (Training: 0.920366, Test: 0.0795962) after 16.0629 seconds\n",
      "2018-09-02 23:08:39.178484 Training Step 545 \"min loss\" =  43.799572\n",
      "2018-09-02 23:08:39.178533 Training Step 545 \"loss\" =  44.958607\n",
      "2018-09-02 23:08:51.424170 Test Step 550 Finished\n",
      "2018-09-02 23:08:51.424257 Test Step 550 \"min loss\" =  116.582504\n",
      "2018-09-02 23:08:51.424295 Test Step 550 \"loss\" =  117.01153\n",
      "2018-09-02 23:08:54.379981 Training Step 550 Finished Timing (Training: 0.920661, Test: 0.0793011) after 15.2014 seconds\n",
      "2018-09-02 23:08:54.380078 Training Step 550 \"min loss\" =  41.46468\n",
      "2018-09-02 23:08:54.380125 Training Step 550 \"loss\" =  44.767044\n",
      "2018-09-02 23:09:06.592335 Test Step 555 Finished\n",
      "2018-09-02 23:09:06.592423 Test Step 555 \"min loss\" =  116.582504\n",
      "2018-09-02 23:09:06.592465 Test Step 555 \"loss\" =  118.92437\n",
      "2018-09-02 23:09:09.556006 Training Step 555 Finished Timing (Training: 0.920553, Test: 0.0794104) after 15.1758 seconds\n",
      "2018-09-02 23:09:09.556105 Training Step 555 \"min loss\" =  38.948414\n",
      "2018-09-02 23:09:09.556152 Training Step 555 \"loss\" =  40.998554\n",
      "2018-09-02 23:09:21.904059 Test Step 560 Finished\n",
      "2018-09-02 23:09:21.904150 Test Step 560 \"min loss\" =  116.582504\n",
      "2018-09-02 23:09:21.904192 Test Step 560 \"loss\" =  120.28607\n",
      "2018-09-02 23:09:24.711511 Training Step 560 Finished Timing (Training: 0.920805, Test: 0.0791592) after 15.1553 seconds\n",
      "2018-09-02 23:09:24.711859 Training Step 560 \"min loss\" =  38.948414\n",
      "2018-09-02 23:09:24.711908 Training Step 560 \"loss\" =  42.888573\n",
      "2018-09-02 23:09:37.421822 Test Step 565 Finished\n",
      "2018-09-02 23:09:37.421929 Test Step 565 \"min loss\" =  116.582504\n",
      "2018-09-02 23:09:37.421977 Test Step 565 \"loss\" =  119.624664\n",
      "2018-09-02 23:09:40.080971 Training Step 565 Finished Timing (Training: 0.920998, Test: 0.0789652) after 15.369 seconds\n",
      "2018-09-02 23:09:40.081095 Training Step 565 \"min loss\" =  38.948414\n",
      "2018-09-02 23:09:40.081147 Training Step 565 \"loss\" =  40.270226\n",
      "2018-09-02 23:09:52.796030 Test Step 570 Finished\n",
      "2018-09-02 23:09:52.796127 Test Step 570 \"min loss\" =  116.582504\n",
      "2018-09-02 23:09:52.796174 Test Step 570 \"loss\" =  118.37757\n",
      "2018-09-02 23:09:55.462073 Training Step 570 Finished Timing (Training: 0.920989, Test: 0.0789745) after 15.3809 seconds\n",
      "2018-09-02 23:09:55.462202 Training Step 570 \"min loss\" =  38.948414\n",
      "2018-09-02 23:09:55.462261 Training Step 570 \"loss\" =  43.151283\n",
      "2018-09-02 23:10:08.212883 Test Step 575 Finished\n",
      "2018-09-02 23:10:08.212976 Test Step 575 \"min loss\" =  116.582504\n",
      "2018-09-02 23:10:08.213015 Test Step 575 \"loss\" =  117.48413\n",
      "2018-09-02 23:10:10.888840 Training Step 575 Finished Timing (Training: 0.920463, Test: 0.0795014) after 15.4265 seconds\n",
      "2018-09-02 23:10:10.888933 Training Step 575 \"min loss\" =  38.948414\n",
      "2018-09-02 23:10:10.888976 Training Step 575 \"loss\" =  40.559357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-09-02 23:10:24.684005 Test Step 580 Finished\n",
      "2018-09-02 23:10:24.684102 Test Step 580 \"min loss\" =  116.582504\n",
      "2018-09-02 23:10:24.684150 Test Step 580 \"loss\" =  118.529724\n",
      "2018-09-02 23:10:27.576661 Training Step 580 Finished Timing (Training: 0.920665, Test: 0.0793001) after 16.6876 seconds\n",
      "2018-09-02 23:10:27.576747 Training Step 580 \"min loss\" =  38.82612\n",
      "2018-09-02 23:10:27.576784 Training Step 580 \"loss\" =  39.475433\n",
      "2018-09-02 23:10:41.682584 Test Step 585 Finished\n",
      "2018-09-02 23:10:41.682700 Test Step 585 \"min loss\" =  116.5423\n",
      "2018-09-02 23:10:41.683082 Test Step 585 \"loss\" =  116.5423\n",
      "2018-09-02 23:10:45.154315 Training Step 585 Finished Timing (Training: 0.920375, Test: 0.0795893) after 17.5775 seconds\n",
      "2018-09-02 23:10:45.154423 Training Step 585 \"min loss\" =  38.82612\n",
      "2018-09-02 23:10:45.154479 Training Step 585 \"loss\" =  39.94512\n",
      "2018-09-02 23:10:59.716179 Test Step 590 Finished\n",
      "2018-09-02 23:10:59.716284 Test Step 590 \"min loss\" =  116.5423\n",
      "2018-09-02 23:10:59.716337 Test Step 590 \"loss\" =  118.25415\n",
      "2018-09-02 23:11:02.672839 Training Step 590 Finished Timing (Training: 0.920141, Test: 0.0798236) after 17.5183 seconds\n",
      "2018-09-02 23:11:02.672963 Training Step 590 \"min loss\" =  38.008663\n",
      "2018-09-02 23:11:02.673029 Training Step 590 \"loss\" =  38.008663\n",
      "2018-09-02 23:11:17.079481 Test Step 595 Finished\n",
      "2018-09-02 23:11:17.079607 Test Step 595 \"min loss\" =  116.5423\n",
      "2018-09-02 23:11:17.079672 Test Step 595 \"loss\" =  117.565544\n",
      "2018-09-02 23:11:20.135733 Training Step 595 Finished Timing (Training: 0.920366, Test: 0.0795995) after 17.4626 seconds\n",
      "2018-09-02 23:11:20.135862 Training Step 595 \"min loss\" =  38.008663\n",
      "2018-09-02 23:11:20.135948 Training Step 595 \"loss\" =  41.59822\n",
      "2018-09-02 23:11:34.396642 Test Step 600 Finished\n",
      "2018-09-02 23:11:34.396731 Test Step 600 \"min loss\" =  115.14455\n",
      "2018-09-02 23:11:34.396767 Test Step 600 \"loss\" =  115.14455\n",
      "2018-09-02 23:11:37.489626 Training Step 600 Finished Timing (Training: 0.920631, Test: 0.0793339) after 17.3536 seconds\n",
      "2018-09-02 23:11:37.489728 Training Step 600 \"min loss\" =  36.546127\n",
      "2018-09-02 23:11:37.489776 Training Step 600 \"loss\" =  41.1502\n",
      "2018-09-02 23:11:50.965830 Test Step 605 Finished\n",
      "2018-09-02 23:11:50.966103 Test Step 605 \"min loss\" =  114.823006\n",
      "2018-09-02 23:11:50.966148 Test Step 605 \"loss\" =  114.823006\n",
      "2018-09-02 23:11:54.093128 Training Step 605 Finished Timing (Training: 0.924374, Test: 0.0756024) after 16.6033 seconds\n",
      "2018-09-02 23:11:54.093380 Training Step 605 \"min loss\" =  36.546127\n",
      "2018-09-02 23:11:54.093440 Training Step 605 \"loss\" =  38.10142\n",
      "2018-09-02 23:12:07.988647 Test Step 610 Finished\n",
      "2018-09-02 23:12:07.988813 Test Step 610 \"min loss\" =  114.823006\n",
      "2018-09-02 23:12:07.988905 Test Step 610 \"loss\" =  115.08162\n",
      "2018-09-02 23:12:11.323312 Training Step 610 Finished Timing (Training: 0.92245, Test: 0.0775165) after 17.2298 seconds\n",
      "2018-09-02 23:12:11.323621 Training Step 610 \"min loss\" =  36.546127\n",
      "2018-09-02 23:12:11.323715 Training Step 610 \"loss\" =  38.29585\n",
      "2018-09-02 23:12:25.140325 Test Step 615 Finished\n",
      "2018-09-02 23:12:25.140475 Test Step 615 \"min loss\" =  114.823006\n",
      "2018-09-02 23:12:25.140601 Test Step 615 \"loss\" =  118.384285\n",
      "2018-09-02 23:12:28.801104 Training Step 615 Finished Timing (Training: 0.916961, Test: 0.0829996) after 17.4773 seconds\n",
      "2018-09-02 23:12:28.801194 Training Step 615 \"min loss\" =  36.546127\n",
      "2018-09-02 23:12:28.801231 Training Step 615 \"loss\" =  38.994377\n",
      "2018-09-02 23:12:42.579513 Test Step 620 Finished\n",
      "2018-09-02 23:12:42.579603 Test Step 620 \"min loss\" =  114.823006\n",
      "2018-09-02 23:12:42.579646 Test Step 620 \"loss\" =  116.53895\n",
      "2018-09-02 23:12:46.081199 Training Step 620 Finished Timing (Training: 0.919077, Test: 0.0808875) after 17.2799 seconds\n",
      "2018-09-02 23:12:46.081292 Training Step 620 \"min loss\" =  35.737545\n",
      "2018-09-02 23:12:46.081339 Training Step 620 \"loss\" =  36.819862\n",
      "2018-09-02 23:13:00.185625 Test Step 625 Finished\n",
      "2018-09-02 23:13:00.185728 Test Step 625 \"min loss\" =  114.62996\n",
      "2018-09-02 23:13:00.185778 Test Step 625 \"loss\" =  114.62996\n",
      "2018-09-02 23:13:03.585216 Training Step 625 Finished Timing (Training: 0.91955, Test: 0.0804172) after 17.5038 seconds\n",
      "2018-09-02 23:13:03.585340 Training Step 625 \"min loss\" =  35.737545\n",
      "2018-09-02 23:13:03.585401 Training Step 625 \"loss\" =  37.038548\n",
      "2018-09-02 23:13:17.504425 Test Step 630 Finished\n",
      "2018-09-02 23:13:17.504548 Test Step 630 \"min loss\" =  114.62996\n",
      "2018-09-02 23:13:17.504613 Test Step 630 \"loss\" =  116.94972\n",
      "2018-09-02 23:13:20.745812 Training Step 630 Finished Timing (Training: 0.919051, Test: 0.0809164) after 17.1603 seconds\n",
      "2018-09-02 23:13:20.745936 Training Step 630 \"min loss\" =  35.729603\n",
      "2018-09-02 23:13:20.746014 Training Step 630 \"loss\" =  37.57226\n",
      "2018-09-02 23:13:33.900733 Test Step 635 Finished\n",
      "2018-09-02 23:13:33.900849 Test Step 635 \"min loss\" =  114.62996\n",
      "2018-09-02 23:13:33.900898 Test Step 635 \"loss\" =  116.16699\n",
      "2018-09-02 23:13:36.737554 Training Step 635 Finished Timing (Training: 0.919188, Test: 0.0807791) after 15.9914 seconds\n",
      "2018-09-02 23:13:36.737654 Training Step 635 \"min loss\" =  34.991447\n",
      "2018-09-02 23:13:36.737705 Training Step 635 \"loss\" =  36.066864\n",
      "2018-09-02 23:13:49.049276 Test Step 640 Finished\n",
      "2018-09-02 23:13:49.049415 Test Step 640 \"min loss\" =  114.62996\n",
      "2018-09-02 23:13:49.049482 Test Step 640 \"loss\" =  115.27131\n",
      "2018-09-02 23:13:51.688968 Training Step 640 Finished Timing (Training: 0.919583, Test: 0.0803846) after 14.9512 seconds\n",
      "2018-09-02 23:13:51.689056 Training Step 640 \"min loss\" =  32.50986\n",
      "2018-09-02 23:13:51.689097 Training Step 640 \"loss\" =  32.50986\n",
      "2018-09-02 23:14:05.788878 Test Step 645 Finished\n",
      "2018-09-02 23:14:05.788989 Test Step 645 \"min loss\" =  114.62996\n",
      "2018-09-02 23:14:05.789037 Test Step 645 \"loss\" =  115.91329\n",
      "2018-09-02 23:14:08.789307 Training Step 645 Finished Timing (Training: 0.920289, Test: 0.0796795) after 17.1002 seconds\n",
      "2018-09-02 23:14:08.789763 Training Step 645 \"min loss\" =  32.28288\n",
      "2018-09-02 23:14:08.789818 Training Step 645 \"loss\" =  34.691177\n",
      "2018-09-02 23:14:22.855115 Test Step 650 Finished\n",
      "2018-09-02 23:14:22.855247 Test Step 650 \"min loss\" =  114.62996\n",
      "2018-09-02 23:14:22.855328 Test Step 650 \"loss\" =  115.05354\n",
      "2018-09-02 23:14:26.112829 Training Step 650 Finished Timing (Training: 0.920702, Test: 0.0792643) after 17.323 seconds\n",
      "2018-09-02 23:14:26.112972 Training Step 650 \"min loss\" =  32.28288\n",
      "2018-09-02 23:14:26.113050 Training Step 650 \"loss\" =  36.331184\n",
      "2018-09-02 23:14:39.663390 Test Step 655 Finished\n",
      "2018-09-02 23:14:39.663477 Test Step 655 \"min loss\" =  114.62996\n",
      "2018-09-02 23:14:39.663513 Test Step 655 \"loss\" =  115.014336\n",
      "2018-09-02 23:14:42.859476 Training Step 655 Finished Timing (Training: 0.920968, Test: 0.0789984) after 16.7463 seconds\n",
      "2018-09-02 23:14:42.859576 Training Step 655 \"min loss\" =  32.28288\n",
      "2018-09-02 23:14:42.859626 Training Step 655 \"loss\" =  32.524193\n",
      "2018-09-02 23:14:57.328270 Test Step 660 Finished\n",
      "2018-09-02 23:14:57.328381 Test Step 660 \"min loss\" =  114.508\n",
      "2018-09-02 23:14:57.328428 Test Step 660 \"loss\" =  114.508\n",
      "2018-09-02 23:15:00.717668 Training Step 660 Finished Timing (Training: 0.921224, Test: 0.0787438) after 17.858 seconds\n",
      "2018-09-02 23:15:00.717816 Training Step 660 \"min loss\" =  32.28288\n",
      "2018-09-02 23:15:00.718077 Training Step 660 \"loss\" =  33.377773\n",
      "2018-09-02 23:15:14.681981 Test Step 665 Finished\n",
      "2018-09-02 23:15:14.682275 Test Step 665 \"min loss\" =  113.40339\n",
      "2018-09-02 23:15:14.682317 Test Step 665 \"loss\" =  113.40339\n",
      "2018-09-02 23:15:17.682816 Training Step 665 Finished Timing (Training: 0.920852, Test: 0.079114) after 16.9646 seconds\n",
      "2018-09-02 23:15:17.682963 Training Step 665 \"min loss\" =  32.28288\n",
      "2018-09-02 23:15:17.683043 Training Step 665 \"loss\" =  34.588627\n",
      "2018-09-02 23:15:31.717662 Test Step 670 Finished\n",
      "2018-09-02 23:15:31.717766 Test Step 670 \"min loss\" =  112.73076\n",
      "2018-09-02 23:15:31.717813 Test Step 670 \"loss\" =  112.73076\n",
      "2018-09-02 23:15:34.542427 Training Step 670 Finished Timing (Training: 0.920438, Test: 0.0795281) after 16.8593 seconds\n",
      "2018-09-02 23:15:34.542515 Training Step 670 \"min loss\" =  31.217403\n",
      "2018-09-02 23:15:34.542557 Training Step 670 \"loss\" =  35.301216\n",
      "2018-09-02 23:15:48.113887 Test Step 675 Finished\n",
      "2018-09-02 23:15:48.113983 Test Step 675 \"min loss\" =  112.73076\n",
      "2018-09-02 23:15:48.114231 Test Step 675 \"loss\" =  113.30523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-09-02 23:15:50.948857 Training Step 675 Finished Timing (Training: 0.920084, Test: 0.0798821) after 16.4063 seconds\n",
      "2018-09-02 23:15:50.948962 Training Step 675 \"min loss\" =  30.113562\n",
      "2018-09-02 23:15:50.949014 Training Step 675 \"loss\" =  32.404984\n",
      "2018-09-02 23:16:02.746065 Test Step 680 Finished\n",
      "2018-09-02 23:16:02.746168 Test Step 680 \"min loss\" =  112.73076\n",
      "2018-09-02 23:16:02.746213 Test Step 680 \"loss\" =  114.44567\n",
      "2018-09-02 23:16:05.448060 Training Step 680 Finished Timing (Training: 0.920231, Test: 0.0797352) after 14.499 seconds\n",
      "2018-09-02 23:16:05.448160 Training Step 680 \"min loss\" =  30.113562\n",
      "2018-09-02 23:16:05.448208 Training Step 680 \"loss\" =  33.373596\n",
      "2018-09-02 23:16:17.939214 Test Step 685 Finished\n",
      "2018-09-02 23:16:17.939305 Test Step 685 \"min loss\" =  112.73076\n",
      "2018-09-02 23:16:17.939341 Test Step 685 \"loss\" =  114.30729\n",
      "2018-09-02 23:16:20.608910 Training Step 685 Finished Timing (Training: 0.920331, Test: 0.0796353) after 15.1607 seconds\n",
      "2018-09-02 23:16:20.609006 Training Step 685 \"min loss\" =  30.113562\n",
      "2018-09-02 23:16:20.609053 Training Step 685 \"loss\" =  33.302013\n",
      "2018-09-02 23:16:33.034381 Test Step 690 Finished\n",
      "2018-09-02 23:16:33.034470 Test Step 690 \"min loss\" =  112.73076\n",
      "2018-09-02 23:16:33.034508 Test Step 690 \"loss\" =  113.980125\n",
      "2018-09-02 23:16:35.644423 Training Step 690 Finished Timing (Training: 0.920501, Test: 0.079466) after 15.0353 seconds\n",
      "2018-09-02 23:16:35.644511 Training Step 690 \"min loss\" =  30.113562\n",
      "2018-09-02 23:16:35.644552 Training Step 690 \"loss\" =  33.898735\n",
      "2018-09-02 23:16:48.048544 Test Step 695 Finished\n",
      "2018-09-02 23:16:48.048646 Test Step 695 \"min loss\" =  112.73076\n",
      "2018-09-02 23:16:48.048694 Test Step 695 \"loss\" =  114.50245\n",
      "2018-09-02 23:16:50.763786 Training Step 695 Finished Timing (Training: 0.920217, Test: 0.0797502) after 15.1192 seconds\n",
      "2018-09-02 23:16:50.763879 Training Step 695 \"min loss\" =  29.978224\n",
      "2018-09-02 23:16:50.763922 Training Step 695 \"loss\" =  33.119316\n",
      "2018-09-02 23:17:02.952349 Test Step 700 Finished\n",
      "2018-09-02 23:17:02.952489 Test Step 700 \"min loss\" =  112.73076\n",
      "2018-09-02 23:17:02.952559 Test Step 700 \"loss\" =  115.56771\n",
      "2018-09-02 23:17:06.155757 Training Step 700 Finished Timing (Training: 0.919633, Test: 0.0803339) after 15.3918 seconds\n",
      "2018-09-02 23:17:06.155869 Training Step 700 \"min loss\" =  29.978224\n",
      "2018-09-02 23:17:06.155934 Training Step 700 \"loss\" =  32.221806\n",
      "2018-09-02 23:17:18.071246 Test Step 705 Finished\n",
      "2018-09-02 23:17:18.071333 Test Step 705 \"min loss\" =  112.73076\n",
      "2018-09-02 23:17:18.071369 Test Step 705 \"loss\" =  115.41284\n",
      "2018-09-02 23:17:20.680274 Training Step 705 Finished Timing (Training: 0.92233, Test: 0.0776576) after 14.5243 seconds\n",
      "2018-09-02 23:17:20.680384 Training Step 705 \"min loss\" =  28.75495\n",
      "2018-09-02 23:17:20.680446 Training Step 705 \"loss\" =  32.307663\n",
      "2018-09-02 23:17:33.184654 Test Step 710 Finished\n",
      "2018-09-02 23:17:33.184746 Test Step 710 \"min loss\" =  112.73076\n",
      "2018-09-02 23:17:33.184787 Test Step 710 \"loss\" =  114.17096\n",
      "2018-09-02 23:17:35.779652 Training Step 710 Finished Timing (Training: 0.923546, Test: 0.0764332) after 15.0991 seconds\n",
      "2018-09-02 23:17:35.779745 Training Step 710 \"min loss\" =  28.75495\n",
      "2018-09-02 23:17:35.779787 Training Step 710 \"loss\" =  29.725092\n",
      "2018-09-02 23:17:48.197972 Test Step 715 Finished\n",
      "2018-09-02 23:17:48.198070 Test Step 715 \"min loss\" =  112.49806\n",
      "2018-09-02 23:17:48.198124 Test Step 715 \"loss\" =  112.49806\n",
      "2018-09-02 23:17:50.807474 Training Step 715 Finished Timing (Training: 0.921229, Test: 0.0787486) after 15.0276 seconds\n",
      "2018-09-02 23:17:50.807564 Training Step 715 \"min loss\" =  28.75495\n",
      "2018-09-02 23:17:50.807606 Training Step 715 \"loss\" =  32.154964\n",
      "2018-09-02 23:18:03.151819 Test Step 720 Finished\n",
      "2018-09-02 23:18:03.152259 Test Step 720 \"min loss\" =  112.49806\n",
      "2018-09-02 23:18:03.152341 Test Step 720 \"loss\" =  112.90624\n",
      "2018-09-02 23:18:05.828345 Training Step 720 Finished Timing (Training: 0.91933, Test: 0.0806394) after 15.0207 seconds\n",
      "2018-09-02 23:18:05.828446 Training Step 720 \"min loss\" =  28.75495\n",
      "2018-09-02 23:18:05.828492 Training Step 720 \"loss\" =  32.903305\n",
      "2018-09-02 23:18:17.870179 Test Step 725 Finished\n",
      "2018-09-02 23:18:17.870274 Test Step 725 \"min loss\" =  112.49806\n",
      "2018-09-02 23:18:17.870316 Test Step 725 \"loss\" =  114.60036\n",
      "2018-09-02 23:18:20.448177 Training Step 725 Finished Timing (Training: 0.920259, Test: 0.0797111) after 14.6196 seconds\n",
      "2018-09-02 23:18:20.448274 Training Step 725 \"min loss\" =  28.75495\n",
      "2018-09-02 23:18:20.448320 Training Step 725 \"loss\" =  33.665607\n",
      "2018-09-02 23:18:33.939599 Test Step 730 Finished\n",
      "2018-09-02 23:18:33.939702 Test Step 730 \"min loss\" =  112.49806\n",
      "2018-09-02 23:18:33.939744 Test Step 730 \"loss\" =  115.83318\n",
      "2018-09-02 23:18:36.815231 Training Step 730 Finished Timing (Training: 0.920975, Test: 0.0789962) after 16.3669 seconds\n",
      "2018-09-02 23:18:36.815343 Training Step 730 \"min loss\" =  27.918543\n",
      "2018-09-02 23:18:36.815397 Training Step 730 \"loss\" =  32.649536\n",
      "2018-09-02 23:18:50.519563 Test Step 735 Finished\n",
      "2018-09-02 23:18:50.519667 Test Step 735 \"min loss\" =  112.49806\n",
      "2018-09-02 23:18:50.519723 Test Step 735 \"loss\" =  114.21143\n",
      "2018-09-02 23:18:53.716193 Training Step 735 Finished Timing (Training: 0.921534, Test: 0.0784376) after 16.9007 seconds\n",
      "2018-09-02 23:18:53.716510 Training Step 735 \"min loss\" =  27.918543\n",
      "2018-09-02 23:18:53.716705 Training Step 735 \"loss\" =  29.059288\n",
      "2018-09-02 23:19:07.201499 Test Step 740 Finished\n",
      "2018-09-02 23:19:07.201588 Test Step 740 \"min loss\" =  112.49806\n",
      "2018-09-02 23:19:07.201631 Test Step 740 \"loss\" =  113.771645\n",
      "2018-09-02 23:19:10.243645 Training Step 740 Finished Timing (Training: 0.920141, Test: 0.0798272) after 16.5269 seconds\n",
      "2018-09-02 23:19:10.243739 Training Step 740 \"min loss\" =  27.865826\n",
      "2018-09-02 23:19:10.243787 Training Step 740 \"loss\" =  31.347284\n",
      "2018-09-02 23:19:23.025180 Test Step 745 Finished\n",
      "2018-09-02 23:19:23.025278 Test Step 745 \"min loss\" =  112.49806\n",
      "2018-09-02 23:19:23.025319 Test Step 745 \"loss\" =  112.67882\n",
      "2018-09-02 23:19:26.344712 Training Step 745 Finished Timing (Training: 0.920277, Test: 0.0796928) after 16.1009 seconds\n",
      "2018-09-02 23:19:26.344825 Training Step 745 \"min loss\" =  27.865826\n",
      "2018-09-02 23:19:26.344893 Training Step 745 \"loss\" =  30.519772\n",
      "2018-09-02 23:19:40.440124 Test Step 750 Finished\n",
      "2018-09-02 23:19:40.440224 Test Step 750 \"min loss\" =  111.99585\n",
      "2018-09-02 23:19:40.440271 Test Step 750 \"loss\" =  111.99585\n",
      "2018-09-02 23:19:43.406307 Training Step 750 Finished Timing (Training: 0.919938, Test: 0.0800318) after 17.0614 seconds\n",
      "2018-09-02 23:19:43.406560 Training Step 750 \"min loss\" =  27.865826\n",
      "2018-09-02 23:19:43.406601 Training Step 750 \"loss\" =  29.369892\n",
      "2018-09-02 23:19:57.736059 Test Step 755 Finished\n",
      "2018-09-02 23:19:57.736352 Test Step 755 \"min loss\" =  111.99585\n",
      "2018-09-02 23:19:57.736392 Test Step 755 \"loss\" =  113.49663\n",
      "2018-09-02 23:20:00.768832 Training Step 755 Finished Timing (Training: 0.91918, Test: 0.0807883) after 17.3622 seconds\n",
      "2018-09-02 23:20:00.768946 Training Step 755 \"min loss\" =  27.576479\n",
      "2018-09-02 23:20:00.769010 Training Step 755 \"loss\" =  27.576479\n",
      "2018-09-02 23:20:14.111169 Test Step 760 Finished\n",
      "2018-09-02 23:20:14.111253 Test Step 760 \"min loss\" =  111.99585\n",
      "2018-09-02 23:20:14.111456 Test Step 760 \"loss\" =  115.08691\n",
      "2018-09-02 23:20:16.935434 Training Step 760 Finished Timing (Training: 0.919497, Test: 0.0804709) after 16.1664 seconds\n",
      "2018-09-02 23:20:16.935516 Training Step 760 \"min loss\" =  27.576479\n",
      "2018-09-02 23:20:16.935556 Training Step 760 \"loss\" =  30.86188\n",
      "2018-09-02 23:20:30.841674 Test Step 765 Finished\n",
      "2018-09-02 23:20:30.841960 Test Step 765 \"min loss\" =  111.99585\n",
      "2018-09-02 23:20:30.842008 Test Step 765 \"loss\" =  114.548874\n",
      "2018-09-02 23:20:33.820338 Training Step 765 Finished Timing (Training: 0.919376, Test: 0.0805924) after 16.8847 seconds\n",
      "2018-09-02 23:20:33.820428 Training Step 765 \"min loss\" =  26.231697\n",
      "2018-09-02 23:20:33.820464 Training Step 765 \"loss\" =  30.694366\n",
      "2018-09-02 23:20:47.442271 Test Step 770 Finished\n",
      "2018-09-02 23:20:47.442396 Test Step 770 \"min loss\" =  111.99585\n",
      "2018-09-02 23:20:47.442460 Test Step 770 \"loss\" =  114.033875\n",
      "2018-09-02 23:20:50.592357 Training Step 770 Finished Timing (Training: 0.918614, Test: 0.0813547) after 16.7719 seconds\n",
      "2018-09-02 23:20:50.592442 Training Step 770 \"min loss\" =  26.231697\n",
      "2018-09-02 23:20:50.592485 Training Step 770 \"loss\" =  29.635317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-09-02 23:21:04.364935 Test Step 775 Finished\n",
      "2018-09-02 23:21:04.365031 Test Step 775 \"min loss\" =  111.99585\n",
      "2018-09-02 23:21:04.365068 Test Step 775 \"loss\" =  114.90948\n",
      "2018-09-02 23:21:07.219769 Training Step 775 Finished Timing (Training: 0.91836, Test: 0.0816088) after 16.6272 seconds\n",
      "2018-09-02 23:21:07.219863 Training Step 775 \"min loss\" =  23.733143\n",
      "2018-09-02 23:21:07.220159 Training Step 775 \"loss\" =  30.023067\n",
      "2018-09-02 23:21:20.948910 Test Step 780 Finished\n",
      "2018-09-02 23:21:20.949007 Test Step 780 \"min loss\" =  111.99585\n",
      "2018-09-02 23:21:20.949052 Test Step 780 \"loss\" =  113.772575\n",
      "2018-09-02 23:21:23.840520 Training Step 780 Finished Timing (Training: 0.91827, Test: 0.0816977) after 16.6203 seconds\n",
      "2018-09-02 23:21:23.840639 Training Step 780 \"min loss\" =  23.733143\n",
      "2018-09-02 23:21:23.840706 Training Step 780 \"loss\" =  30.436132\n",
      "2018-09-02 23:21:37.505288 Test Step 785 Finished\n",
      "2018-09-02 23:21:37.505412 Test Step 785 \"min loss\" =  111.99585\n",
      "2018-09-02 23:21:37.505483 Test Step 785 \"loss\" =  113.98769\n",
      "2018-09-02 23:21:40.372683 Training Step 785 Finished Timing (Training: 0.91845, Test: 0.0815182) after 16.5319 seconds\n",
      "2018-09-02 23:21:40.372788 Training Step 785 \"min loss\" =  23.733143\n",
      "2018-09-02 23:21:40.373140 Training Step 785 \"loss\" =  27.022448\n",
      "2018-09-02 23:21:54.030337 Test Step 790 Finished\n",
      "2018-09-02 23:21:54.030431 Test Step 790 \"min loss\" =  111.99585\n",
      "2018-09-02 23:21:54.030475 Test Step 790 \"loss\" =  114.15406\n",
      "2018-09-02 23:21:56.974284 Training Step 790 Finished Timing (Training: 0.918828, Test: 0.0811392) after 16.6011 seconds\n",
      "2018-09-02 23:21:56.974400 Training Step 790 \"min loss\" =  23.733143\n",
      "2018-09-02 23:21:56.974483 Training Step 790 \"loss\" =  27.876776\n",
      "2018-09-02 23:22:10.569890 Test Step 795 Finished\n",
      "2018-09-02 23:22:10.570010 Test Step 795 \"min loss\" =  111.99585\n",
      "2018-09-02 23:22:10.570062 Test Step 795 \"loss\" =  115.02881\n",
      "2018-09-02 23:22:13.534278 Training Step 795 Finished Timing (Training: 0.919117, Test: 0.0808501) after 16.5597 seconds\n",
      "2018-09-02 23:22:13.534373 Training Step 795 \"min loss\" =  23.733143\n",
      "2018-09-02 23:22:13.534421 Training Step 795 \"loss\" =  27.263365\n",
      "2018-09-02 23:22:26.776106 Test Step 800 Finished\n",
      "2018-09-02 23:22:26.776192 Test Step 800 \"min loss\" =  111.99585\n",
      "2018-09-02 23:22:26.776227 Test Step 800 \"loss\" =  113.60147\n",
      "2018-09-02 23:22:29.770738 Training Step 800 Finished Timing (Training: 0.919384, Test: 0.0805838) after 16.2363 seconds\n",
      "2018-09-02 23:22:29.770830 Training Step 800 \"min loss\" =  23.733143\n",
      "2018-09-02 23:22:29.770871 Training Step 800 \"loss\" =  25.50179\n",
      "2018-09-02 23:22:43.258976 Test Step 805 Finished\n",
      "2018-09-02 23:22:43.259073 Test Step 805 \"min loss\" =  111.99585\n",
      "2018-09-02 23:22:43.259116 Test Step 805 \"loss\" =  113.583595\n",
      "2018-09-02 23:22:46.686399 Training Step 805 Finished Timing (Training: 0.924053, Test: 0.0759352) after 16.9155 seconds\n",
      "2018-09-02 23:22:46.686567 Training Step 805 \"min loss\" =  23.733143\n",
      "2018-09-02 23:22:46.686641 Training Step 805 \"loss\" =  24.105902\n",
      "2018-09-02 23:22:59.656174 Test Step 810 Finished\n",
      "2018-09-02 23:22:59.656272 Test Step 810 \"min loss\" =  111.99585\n",
      "2018-09-02 23:22:59.656484 Test Step 810 \"loss\" =  114.037834\n",
      "2018-09-02 23:23:02.747782 Training Step 810 Finished Timing (Training: 0.923611, Test: 0.0763624) after 16.0611 seconds\n",
      "2018-09-02 23:23:02.747890 Training Step 810 \"min loss\" =  23.733143\n",
      "2018-09-02 23:23:02.747944 Training Step 810 \"loss\" =  25.034729\n",
      "2018-09-02 23:23:15.462273 Test Step 815 Finished\n",
      "2018-09-02 23:23:15.462650 Test Step 815 \"min loss\" =  111.99585\n",
      "2018-09-02 23:23:15.462693 Test Step 815 \"loss\" =  114.42868\n",
      "2018-09-02 23:23:18.161996 Training Step 815 Finished Timing (Training: 0.92179, Test: 0.0781769) after 15.414 seconds\n",
      "2018-09-02 23:23:18.162087 Training Step 815 \"min loss\" =  23.733143\n",
      "2018-09-02 23:23:18.162138 Training Step 815 \"loss\" =  29.155287\n",
      "2018-09-02 23:23:31.074158 Test Step 820 Finished\n",
      "2018-09-02 23:23:31.074289 Test Step 820 \"min loss\" =  111.99585\n",
      "2018-09-02 23:23:31.074352 Test Step 820 \"loss\" =  113.35031\n",
      "2018-09-02 23:23:34.021086 Training Step 820 Finished Timing (Training: 0.919678, Test: 0.0802905) after 15.8589 seconds\n",
      "2018-09-02 23:23:34.021191 Training Step 820 \"min loss\" =  23.733143\n",
      "2018-09-02 23:23:34.021248 Training Step 820 \"loss\" =  25.84154\n",
      "2018-09-02 23:23:48.207315 Test Step 825 Finished\n",
      "2018-09-02 23:23:48.207415 Test Step 825 \"min loss\" =  111.99585\n",
      "2018-09-02 23:23:48.207466 Test Step 825 \"loss\" =  113.1266\n",
      "2018-09-02 23:23:51.039439 Training Step 825 Finished Timing (Training: 0.919464, Test: 0.0805054) after 17.0181 seconds\n",
      "2018-09-02 23:23:51.039535 Training Step 825 \"min loss\" =  23.733143\n",
      "2018-09-02 23:23:51.039588 Training Step 825 \"loss\" =  25.482147\n",
      "2018-09-02 23:24:04.632616 Test Step 830 Finished\n",
      "2018-09-02 23:24:04.632711 Test Step 830 \"min loss\" =  111.99585\n",
      "2018-09-02 23:24:04.632752 Test Step 830 \"loss\" =  112.777985\n",
      "2018-09-02 23:24:07.435963 Training Step 830 Finished Timing (Training: 0.918756, Test: 0.0812139) after 16.3963 seconds\n",
      "2018-09-02 23:24:07.436051 Training Step 830 \"min loss\" =  23.733143\n",
      "2018-09-02 23:24:07.436239 Training Step 830 \"loss\" =  27.866907\n",
      "2018-09-02 23:24:20.360075 Test Step 835 Finished\n",
      "2018-09-02 23:24:20.360158 Test Step 835 \"min loss\" =  111.99585\n",
      "2018-09-02 23:24:20.360426 Test Step 835 \"loss\" =  112.41559\n",
      "2018-09-02 23:24:23.425251 Training Step 835 Finished Timing (Training: 0.919534, Test: 0.0804334) after 15.989 seconds\n",
      "2018-09-02 23:24:23.425367 Training Step 835 \"min loss\" =  23.733143\n",
      "2018-09-02 23:24:23.425429 Training Step 835 \"loss\" =  25.59736\n",
      "2018-09-02 23:24:39.108295 Test Step 840 Finished\n",
      "2018-09-02 23:24:39.108734 Test Step 840 \"min loss\" =  111.99585\n",
      "2018-09-02 23:24:39.108782 Test Step 840 \"loss\" =  112.608055\n",
      "2018-09-02 23:24:42.362051 Training Step 840 Finished Timing (Training: 0.919626, Test: 0.08034) after 18.9366 seconds\n",
      "2018-09-02 23:24:42.362151 Training Step 840 \"min loss\" =  23.733143\n",
      "2018-09-02 23:24:42.362193 Training Step 840 \"loss\" =  24.56149\n",
      "2018-09-02 23:24:54.291134 Test Step 845 Finished\n",
      "2018-09-02 23:24:54.291450 Test Step 845 \"min loss\" =  111.99585\n",
      "2018-09-02 23:24:54.291504 Test Step 845 \"loss\" =  113.62394\n",
      "2018-09-02 23:24:57.395402 Training Step 845 Finished Timing (Training: 0.918176, Test: 0.0817892) after 15.0332 seconds\n",
      "2018-09-02 23:24:57.395500 Training Step 845 \"min loss\" =  23.733143\n",
      "2018-09-02 23:24:57.395547 Training Step 845 \"loss\" =  29.656685\n",
      "2018-09-02 23:25:10.285496 Test Step 850 Finished\n",
      "2018-09-02 23:25:10.285611 Test Step 850 \"min loss\" =  111.99585\n",
      "2018-09-02 23:25:10.285684 Test Step 850 \"loss\" =  113.464134\n",
      "2018-09-02 23:25:13.481847 Training Step 850 Finished Timing (Training: 0.918463, Test: 0.0815026) after 16.0862 seconds\n",
      "2018-09-02 23:25:13.481981 Training Step 850 \"min loss\" =  23.200306\n",
      "2018-09-02 23:25:13.482060 Training Step 850 \"loss\" =  23.200306\n",
      "2018-09-02 23:25:27.560158 Test Step 855 Finished\n",
      "2018-09-02 23:25:27.560261 Test Step 855 \"min loss\" =  111.98518\n",
      "2018-09-02 23:25:27.560316 Test Step 855 \"loss\" =  111.98518\n",
      "2018-09-02 23:25:30.787196 Training Step 855 Finished Timing (Training: 0.919007, Test: 0.080959) after 17.305 seconds\n",
      "2018-09-02 23:25:30.787310 Training Step 855 \"min loss\" =  23.200306\n",
      "2018-09-02 23:25:30.787370 Training Step 855 \"loss\" =  24.464039\n",
      "2018-09-02 23:25:44.595572 Test Step 860 Finished\n",
      "2018-09-02 23:25:44.595657 Test Step 860 \"min loss\" =  111.98518\n",
      "2018-09-02 23:25:44.595693 Test Step 860 \"loss\" =  113.50521\n",
      "2018-09-02 23:25:47.797230 Training Step 860 Finished Timing (Training: 0.919379, Test: 0.0805882) after 17.0098 seconds\n",
      "2018-09-02 23:25:47.797496 Training Step 860 \"min loss\" =  23.200306\n",
      "2018-09-02 23:25:47.797552 Training Step 860 \"loss\" =  24.607044\n",
      "2018-09-02 23:26:01.428795 Test Step 865 Finished\n",
      "2018-09-02 23:26:01.428892 Test Step 865 \"min loss\" =  111.98518\n",
      "2018-09-02 23:26:01.428939 Test Step 865 \"loss\" =  113.09239\n",
      "2018-09-02 23:26:04.774485 Training Step 865 Finished Timing (Training: 0.919456, Test: 0.0805112) after 16.9769 seconds\n",
      "2018-09-02 23:26:04.774582 Training Step 865 \"min loss\" =  22.487627\n",
      "2018-09-02 23:26:04.774624 Training Step 865 \"loss\" =  24.89721\n",
      "2018-09-02 23:26:19.027896 Test Step 870 Finished\n",
      "2018-09-02 23:26:19.028006 Test Step 870 \"min loss\" =  111.98518\n",
      "2018-09-02 23:26:19.028060 Test Step 870 \"loss\" =  113.96632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-09-02 23:26:22.162682 Training Step 870 Finished Timing (Training: 0.919406, Test: 0.0805618) after 17.388 seconds\n",
      "2018-09-02 23:26:22.162774 Training Step 870 \"min loss\" =  22.487627\n",
      "2018-09-02 23:26:22.162817 Training Step 870 \"loss\" =  23.51291\n",
      "2018-09-02 23:26:35.375889 Test Step 875 Finished\n",
      "2018-09-02 23:26:35.375984 Test Step 875 \"min loss\" =  111.98518\n",
      "2018-09-02 23:26:35.376022 Test Step 875 \"loss\" =  112.96143\n",
      "2018-09-02 23:26:38.198063 Training Step 875 Finished Timing (Training: 0.919546, Test: 0.0804216) after 16.0352 seconds\n",
      "2018-09-02 23:26:38.198163 Training Step 875 \"min loss\" =  21.726503\n",
      "2018-09-02 23:26:38.198215 Training Step 875 \"loss\" =  27.526083\n",
      "2018-09-02 23:26:52.562234 Test Step 880 Finished\n",
      "2018-09-02 23:26:52.562542 Test Step 880 \"min loss\" =  111.98518\n",
      "2018-09-02 23:26:52.562633 Test Step 880 \"loss\" =  114.31242\n",
      "2018-09-02 23:26:55.986219 Training Step 880 Finished Timing (Training: 0.92006, Test: 0.079907) after 17.7879 seconds\n",
      "2018-09-02 23:26:55.986344 Training Step 880 \"min loss\" =  21.726503\n",
      "2018-09-02 23:26:55.986411 Training Step 880 \"loss\" =  25.26958\n",
      "2018-09-02 23:27:09.624084 Test Step 885 Finished\n",
      "2018-09-02 23:27:09.624177 Test Step 885 \"min loss\" =  111.98518\n",
      "2018-09-02 23:27:09.624219 Test Step 885 \"loss\" =  113.33963\n",
      "2018-09-02 23:27:12.817065 Training Step 885 Finished Timing (Training: 0.919917, Test: 0.0800508) after 16.8306 seconds\n",
      "2018-09-02 23:27:12.817166 Training Step 885 \"min loss\" =  21.726503\n",
      "2018-09-02 23:27:12.817230 Training Step 885 \"loss\" =  23.88156\n",
      "2018-09-02 23:27:26.455179 Test Step 890 Finished\n",
      "2018-09-02 23:27:26.455295 Test Step 890 \"min loss\" =  111.98518\n",
      "2018-09-02 23:27:26.455354 Test Step 890 \"loss\" =  112.74183\n",
      "2018-09-02 23:27:29.624611 Training Step 890 Finished Timing (Training: 0.919645, Test: 0.0803231) after 16.8073 seconds\n",
      "2018-09-02 23:27:29.624712 Training Step 890 \"min loss\" =  21.726503\n",
      "2018-09-02 23:27:29.624769 Training Step 890 \"loss\" =  23.994478\n",
      "2018-09-02 23:27:42.449780 Test Step 895 Finished\n",
      "2018-09-02 23:27:42.449912 Test Step 895 \"min loss\" =  111.98518\n",
      "2018-09-02 23:27:42.449978 Test Step 895 \"loss\" =  112.81925\n",
      "2018-09-02 23:27:45.031519 Training Step 895 Finished Timing (Training: 0.919963, Test: 0.0800045) after 15.4067 seconds\n",
      "2018-09-02 23:27:45.031605 Training Step 895 \"min loss\" =  21.726503\n",
      "2018-09-02 23:27:45.031642 Training Step 895 \"loss\" =  22.88018\n",
      "2018-09-02 23:27:59.082036 Test Step 900 Finished\n",
      "2018-09-02 23:27:59.082149 Test Step 900 \"min loss\" =  111.98518\n",
      "2018-09-02 23:27:59.082463 Test Step 900 \"loss\" =  113.32455\n",
      "2018-09-02 23:28:02.262176 Training Step 900 Finished Timing (Training: 0.920348, Test: 0.0796191) after 17.2303 seconds\n",
      "2018-09-02 23:28:02.262288 Training Step 900 \"min loss\" =  21.726503\n",
      "2018-09-02 23:28:02.262350 Training Step 900 \"loss\" =  23.151398\n",
      "2018-09-02 23:28:16.463467 Test Step 905 Finished\n",
      "2018-09-02 23:28:16.463562 Test Step 905 \"min loss\" =  111.98518\n",
      "2018-09-02 23:28:16.463605 Test Step 905 \"loss\" =  113.92756\n",
      "2018-09-02 23:28:19.618288 Training Step 905 Finished Timing (Training: 0.923607, Test: 0.0763682) after 17.3559 seconds\n",
      "2018-09-02 23:28:19.618406 Training Step 905 \"min loss\" =  21.726503\n",
      "2018-09-02 23:28:19.618472 Training Step 905 \"loss\" =  24.17637\n",
      "2018-09-02 23:28:32.196356 Test Step 910 Finished\n",
      "2018-09-02 23:28:32.196455 Test Step 910 \"min loss\" =  111.98518\n",
      "2018-09-02 23:28:32.196496 Test Step 910 \"loss\" =  113.33378\n",
      "2018-09-02 23:28:35.298639 Training Step 910 Finished Timing (Training: 0.924065, Test: 0.0759084) after 15.6801 seconds\n",
      "2018-09-02 23:28:35.298738 Training Step 910 \"min loss\" =  21.726503\n",
      "2018-09-02 23:28:35.298787 Training Step 910 \"loss\" =  23.933937\n",
      "2018-09-02 23:28:47.565123 Test Step 915 Finished\n",
      "2018-09-02 23:28:47.565212 Test Step 915 \"min loss\" =  111.98518\n",
      "2018-09-02 23:28:47.565250 Test Step 915 \"loss\" =  113.03192\n",
      "2018-09-02 23:28:50.155725 Training Step 915 Finished Timing (Training: 0.924072, Test: 0.0758955) after 14.8569 seconds\n",
      "2018-09-02 23:28:50.155813 Training Step 915 \"min loss\" =  21.726503\n",
      "2018-09-02 23:28:50.155853 Training Step 915 \"loss\" =  23.885666\n",
      "2018-09-02 23:29:03.011101 Test Step 920 Finished\n",
      "2018-09-02 23:29:03.011234 Test Step 920 \"min loss\" =  111.98518\n",
      "2018-09-02 23:29:03.011306 Test Step 920 \"loss\" =  113.14343\n",
      "2018-09-02 23:29:05.790304 Training Step 920 Finished Timing (Training: 0.922427, Test: 0.0775409) after 15.6344 seconds\n",
      "2018-09-02 23:29:05.790414 Training Step 920 \"min loss\" =  21.726503\n",
      "2018-09-02 23:29:05.790473 Training Step 920 \"loss\" =  22.69603\n",
      "2018-09-02 23:29:18.768247 Test Step 925 Finished\n",
      "2018-09-02 23:29:18.768555 Test Step 925 \"min loss\" =  111.98518\n",
      "2018-09-02 23:29:18.768602 Test Step 925 \"loss\" =  112.788864\n",
      "2018-09-02 23:29:21.799209 Training Step 925 Finished Timing (Training: 0.921989, Test: 0.0779772) after 16.0087 seconds\n",
      "2018-09-02 23:29:21.799296 Training Step 925 \"min loss\" =  21.214134\n",
      "2018-09-02 23:29:21.799338 Training Step 925 \"loss\" =  22.61021\n",
      "2018-09-02 23:29:34.292214 Test Step 930 Finished\n",
      "2018-09-02 23:29:34.292558 Test Step 930 \"min loss\" =  111.98518\n",
      "2018-09-02 23:29:34.292601 Test Step 930 \"loss\" =  113.07243\n",
      "2018-09-02 23:29:37.195180 Training Step 930 Finished Timing (Training: 0.92128, Test: 0.0786857) after 15.3958 seconds\n",
      "2018-09-02 23:29:37.195272 Training Step 930 \"min loss\" =  21.214134\n",
      "2018-09-02 23:29:37.195314 Training Step 930 \"loss\" =  24.855793\n",
      "2018-09-02 23:29:49.364753 Test Step 935 Finished\n",
      "2018-09-02 23:29:49.364851 Test Step 935 \"min loss\" =  111.98518\n",
      "2018-09-02 23:29:49.365144 Test Step 935 \"loss\" =  112.20072\n",
      "2018-09-02 23:29:52.325771 Training Step 935 Finished Timing (Training: 0.921042, Test: 0.0789224) after 15.1304 seconds\n",
      "2018-09-02 23:29:52.325878 Training Step 935 \"min loss\" =  21.214134\n",
      "2018-09-02 23:29:52.325930 Training Step 935 \"loss\" =  22.21695\n",
      "2018-09-02 23:30:04.476516 Test Step 940 Finished\n",
      "2018-09-02 23:30:04.476602 Test Step 940 \"min loss\" =  111.98518\n",
      "2018-09-02 23:30:04.476870 Test Step 940 \"loss\" =  112.590256\n",
      "2018-09-02 23:30:07.509188 Training Step 940 Finished Timing (Training: 0.921571, Test: 0.0783923) after 15.1832 seconds\n",
      "2018-09-02 23:30:07.509285 Training Step 940 \"min loss\" =  21.214134\n",
      "2018-09-02 23:30:07.509333 Training Step 940 \"loss\" =  22.107908\n",
      "2018-09-02 23:30:20.851796 Test Step 945 Finished\n",
      "2018-09-02 23:30:20.852180 Test Step 945 \"min loss\" =  111.98518\n",
      "2018-09-02 23:30:20.852224 Test Step 945 \"loss\" =  113.60767\n",
      "2018-09-02 23:30:24.199684 Training Step 945 Finished Timing (Training: 0.922205, Test: 0.0777573) after 16.6903 seconds\n",
      "2018-09-02 23:30:24.199808 Training Step 945 \"min loss\" =  20.56961\n",
      "2018-09-02 23:30:24.199875 Training Step 945 \"loss\" =  22.747585\n",
      "2018-09-02 23:30:37.469947 Test Step 950 Finished\n",
      "2018-09-02 23:30:37.470137 Test Step 950 \"min loss\" =  111.98518\n",
      "2018-09-02 23:30:37.470239 Test Step 950 \"loss\" =  113.69391\n",
      "2018-09-02 23:30:40.697186 Training Step 950 Finished Timing (Training: 0.922093, Test: 0.0778695) after 16.4972 seconds\n",
      "2018-09-02 23:30:40.697503 Training Step 950 \"min loss\" =  20.56961\n",
      "2018-09-02 23:30:40.697597 Training Step 950 \"loss\" =  24.303785\n",
      "2018-09-02 23:30:54.096568 Test Step 955 Finished\n",
      "2018-09-02 23:30:54.096872 Test Step 955 \"min loss\" =  111.98518\n",
      "2018-09-02 23:30:54.096915 Test Step 955 \"loss\" =  113.26815\n",
      "2018-09-02 23:30:57.005585 Training Step 955 Finished Timing (Training: 0.922575, Test: 0.0773856) after 16.3079 seconds\n",
      "2018-09-02 23:30:57.005684 Training Step 955 \"min loss\" =  20.56961\n",
      "2018-09-02 23:30:57.005728 Training Step 955 \"loss\" =  24.480759\n",
      "2018-09-02 23:31:11.032911 Test Step 960 Finished\n",
      "2018-09-02 23:31:11.033008 Test Step 960 \"min loss\" =  111.98518\n",
      "2018-09-02 23:31:11.033050 Test Step 960 \"loss\" =  112.21174\n",
      "2018-09-02 23:31:13.956853 Training Step 960 Finished Timing (Training: 0.92289, Test: 0.0770716) after 16.9511 seconds\n",
      "2018-09-02 23:31:13.956993 Training Step 960 \"min loss\" =  20.56961\n",
      "2018-09-02 23:31:13.957076 Training Step 960 \"loss\" =  22.97025\n",
      "2018-09-02 23:31:27.770041 Test Step 965 Finished\n",
      "2018-09-02 23:31:27.770153 Test Step 965 \"min loss\" =  111.89674\n",
      "2018-09-02 23:31:27.770194 Test Step 965 \"loss\" =  111.89674\n",
      "2018-09-02 23:31:30.972390 Training Step 965 Finished Timing (Training: 0.922985, Test: 0.0769776) after 17.0152 seconds\n",
      "2018-09-02 23:31:30.972488 Training Step 965 \"min loss\" =  20.56961\n",
      "2018-09-02 23:31:30.972535 Training Step 965 \"loss\" =  22.477718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-09-02 23:31:44.541279 Test Step 970 Finished\n",
      "2018-09-02 23:31:44.541367 Test Step 970 \"min loss\" =  111.89674\n",
      "2018-09-02 23:31:44.541405 Test Step 970 \"loss\" =  113.54352\n",
      "2018-09-02 23:31:47.689611 Training Step 970 Finished Timing (Training: 0.922997, Test: 0.0769662) after 16.717 seconds\n",
      "2018-09-02 23:31:47.689872 Training Step 970 \"min loss\" =  20.1563\n",
      "2018-09-02 23:31:47.689916 Training Step 970 \"loss\" =  22.64193\n",
      "2018-09-02 23:32:00.688534 Test Step 975 Finished\n",
      "2018-09-02 23:32:00.688632 Test Step 975 \"min loss\" =  111.89674\n",
      "2018-09-02 23:32:00.688681 Test Step 975 \"loss\" =  112.33209\n",
      "2018-09-02 23:32:03.774831 Training Step 975 Finished Timing (Training: 0.922985, Test: 0.076978) after 16.0849 seconds\n",
      "2018-09-02 23:32:03.774921 Training Step 975 \"min loss\" =  20.1563\n",
      "2018-09-02 23:32:03.774966 Training Step 975 \"loss\" =  23.955683\n",
      "2018-09-02 23:32:16.681731 Test Step 980 Finished\n",
      "2018-09-02 23:32:16.681818 Test Step 980 \"min loss\" =  111.89674\n",
      "2018-09-02 23:32:16.681860 Test Step 980 \"loss\" =  113.398056\n",
      "2018-09-02 23:32:19.752715 Training Step 980 Finished Timing (Training: 0.92301, Test: 0.0769542) after 15.9777 seconds\n",
      "2018-09-02 23:32:19.752858 Training Step 980 \"min loss\" =  20.152826\n",
      "2018-09-02 23:32:19.752949 Training Step 980 \"loss\" =  23.715994\n",
      "2018-09-02 23:32:32.724264 Test Step 985 Finished\n",
      "2018-09-02 23:32:32.724350 Test Step 985 \"min loss\" =  111.89674\n",
      "2018-09-02 23:32:32.724390 Test Step 985 \"loss\" =  115.06852\n",
      "2018-09-02 23:32:35.815922 Training Step 985 Finished Timing (Training: 0.923061, Test: 0.0769034) after 16.0629 seconds\n",
      "2018-09-02 23:32:35.816226 Training Step 985 \"min loss\" =  20.152826\n",
      "2018-09-02 23:32:35.816273 Training Step 985 \"loss\" =  21.795782\n",
      "2018-09-02 23:32:49.629616 Test Step 990 Finished\n",
      "2018-09-02 23:32:49.629727 Test Step 990 \"min loss\" =  111.89674\n",
      "2018-09-02 23:32:49.629791 Test Step 990 \"loss\" =  113.24198\n",
      "2018-09-02 23:32:52.786936 Training Step 990 Finished Timing (Training: 0.922857, Test: 0.0771064) after 16.9705 seconds\n",
      "2018-09-02 23:32:52.787028 Training Step 990 \"min loss\" =  20.152826\n",
      "2018-09-02 23:32:52.787070 Training Step 990 \"loss\" =  21.4829\n",
      "2018-09-02 23:33:05.358358 Test Step 995 Finished\n",
      "2018-09-02 23:33:05.358461 Test Step 995 \"min loss\" =  111.89674\n",
      "2018-09-02 23:33:05.358511 Test Step 995 \"loss\" =  112.7182\n",
      "2018-09-02 23:33:08.198809 Training Step 995 Finished Timing (Training: 0.922977, Test: 0.0769873) after 15.4117 seconds\n",
      "2018-09-02 23:33:08.198906 Training Step 995 \"min loss\" =  20.152826\n",
      "2018-09-02 23:33:08.198953 Training Step 995 \"loss\" =  22.757828\n",
      "2018-09-02 23:33:20.183455 Test Step 1000 Finished\n",
      "2018-09-02 23:33:20.183556 Test Step 1000 \"min loss\" =  111.89674\n",
      "2018-09-02 23:33:20.183608 Test Step 1000 \"loss\" =  113.12898\n",
      "2018-09-02 23:33:23.022998 Training Step 1000 Finished Timing (Training: 0.922969, Test: 0.0769955) after 14.824 seconds\n",
      "2018-09-02 23:33:23.023253 Training Step 1000 \"min loss\" =  19.649736\n",
      "2018-09-02 23:33:23.023305 Training Step 1000 \"loss\" =  20.9558\n",
      "2018-09-02 23:33:35.102274 Test Step 1005 Finished\n",
      "2018-09-02 23:33:35.102371 Test Step 1005 \"min loss\" =  111.89674\n",
      "2018-09-02 23:33:35.102410 Test Step 1005 \"loss\" =  113.47487\n",
      "2018-09-02 23:33:37.836504 Training Step 1005 Finished Timing (Training: 0.923053, Test: 0.0769329) after 14.8131 seconds\n",
      "2018-09-02 23:33:37.836611 Training Step 1005 \"min loss\" =  19.518534\n",
      "2018-09-02 23:33:37.836663 Training Step 1005 \"loss\" =  23.081524\n",
      "2018-09-02 23:33:50.261493 Test Step 1010 Finished\n",
      "2018-09-02 23:33:50.261585 Test Step 1010 \"min loss\" =  111.89674\n",
      "2018-09-02 23:33:50.261641 Test Step 1010 \"loss\" =  113.31991\n",
      "2018-09-02 23:33:52.861464 Training Step 1010 Finished Timing (Training: 0.920039, Test: 0.0799339) after 15.0247 seconds\n",
      "2018-09-02 23:33:52.861554 Training Step 1010 \"min loss\" =  19.518534\n",
      "2018-09-02 23:33:52.861596 Training Step 1010 \"loss\" =  20.942095\n",
      "2018-09-02 23:34:05.240060 Test Step 1015 Finished\n",
      "2018-09-02 23:34:05.240159 Test Step 1015 \"min loss\" =  111.89674\n",
      "2018-09-02 23:34:05.240199 Test Step 1015 \"loss\" =  113.51878\n",
      "2018-09-02 23:34:07.784495 Training Step 1015 Finished Timing (Training: 0.919261, Test: 0.080712) after 14.9229 seconds\n",
      "2018-09-02 23:34:07.784586 Training Step 1015 \"min loss\" =  18.986729\n",
      "2018-09-02 23:34:07.784628 Training Step 1015 \"loss\" =  23.621227\n",
      "2018-09-02 23:34:20.104037 Test Step 1020 Finished\n",
      "2018-09-02 23:34:20.104140 Test Step 1020 \"min loss\" =  111.89674\n",
      "2018-09-02 23:34:20.104186 Test Step 1020 \"loss\" =  113.86855\n",
      "2018-09-02 23:34:22.785825 Training Step 1020 Finished Timing (Training: 0.918713, Test: 0.0812565) after 15.0012 seconds\n",
      "2018-09-02 23:34:22.785913 Training Step 1020 \"min loss\" =  18.986729\n",
      "2018-09-02 23:34:22.785955 Training Step 1020 \"loss\" =  22.546295\n",
      "2018-09-02 23:34:35.011080 Test Step 1025 Finished\n",
      "2018-09-02 23:34:35.011196 Test Step 1025 \"min loss\" =  111.89674\n",
      "2018-09-02 23:34:35.011259 Test Step 1025 \"loss\" =  113.85292\n",
      "2018-09-02 23:34:37.800067 Training Step 1025 Finished Timing (Training: 0.917826, Test: 0.082144) after 15.0141 seconds\n",
      "2018-09-02 23:34:37.800155 Training Step 1025 \"min loss\" =  18.986729\n",
      "2018-09-02 23:34:37.800197 Training Step 1025 \"loss\" =  23.602858\n",
      "2018-09-02 23:34:50.053332 Test Step 1030 Finished\n",
      "2018-09-02 23:34:50.053429 Test Step 1030 \"min loss\" =  111.89674\n",
      "2018-09-02 23:34:50.053471 Test Step 1030 \"loss\" =  113.28245\n",
      "2018-09-02 23:34:53.083247 Training Step 1030 Finished Timing (Training: 0.917571, Test: 0.0823974) after 15.283 seconds\n",
      "2018-09-02 23:34:53.083407 Training Step 1030 \"min loss\" =  18.986729\n",
      "2018-09-02 23:34:53.083503 Training Step 1030 \"loss\" =  22.2758\n",
      "2018-09-02 23:35:05.665374 Test Step 1035 Finished\n",
      "2018-09-02 23:35:05.665683 Test Step 1035 \"min loss\" =  111.89674\n",
      "2018-09-02 23:35:05.665739 Test Step 1035 \"loss\" =  112.26201\n",
      "2018-09-02 23:35:08.834911 Training Step 1035 Finished Timing (Training: 0.917445, Test: 0.0825203) after 15.7513 seconds\n",
      "2018-09-02 23:35:08.835213 Training Step 1035 \"min loss\" =  18.986729\n",
      "2018-09-02 23:35:08.835261 Training Step 1035 \"loss\" =  20.083803\n",
      "2018-09-02 23:35:22.136122 Test Step 1040 Finished\n",
      "2018-09-02 23:35:22.136214 Test Step 1040 \"min loss\" =  111.89674\n",
      "2018-09-02 23:35:22.136475 Test Step 1040 \"loss\" =  112.73587\n",
      "2018-09-02 23:35:25.271837 Training Step 1040 Finished Timing (Training: 0.91721, Test: 0.0827534) after 16.4365 seconds\n",
      "2018-09-02 23:35:25.271921 Training Step 1040 \"min loss\" =  18.986729\n",
      "2018-09-02 23:35:25.271957 Training Step 1040 \"loss\" =  19.65817\n",
      "2018-09-02 23:35:38.474715 Test Step 1045 Finished\n",
      "2018-09-02 23:35:38.474955 Test Step 1045 \"min loss\" =  111.89674\n",
      "2018-09-02 23:35:38.474992 Test Step 1045 \"loss\" =  113.99321\n",
      "2018-09-02 23:35:41.376699 Training Step 1045 Finished Timing (Training: 0.91718, Test: 0.0827833) after 16.1047 seconds\n",
      "2018-09-02 23:35:41.376790 Training Step 1045 \"min loss\" =  18.986729\n",
      "2018-09-02 23:35:41.376833 Training Step 1045 \"loss\" =  20.170807\n",
      "2018-09-02 23:35:57.000071 Test Step 1050 Finished\n",
      "2018-09-02 23:35:57.000427 Test Step 1050 \"min loss\" =  111.89674\n",
      "2018-09-02 23:35:57.000545 Test Step 1050 \"loss\" =  113.64136\n",
      "2018-09-02 23:36:00.227876 Training Step 1050 Finished Timing (Training: 0.918065, Test: 0.0818988) after 18.851 seconds\n",
      "2018-09-02 23:36:00.228037 Training Step 1050 \"min loss\" =  18.986729\n",
      "2018-09-02 23:36:00.228131 Training Step 1050 \"loss\" =  22.119202\n",
      "2018-09-02 23:36:13.969508 Test Step 1055 Finished\n",
      "2018-09-02 23:36:13.969783 Test Step 1055 \"min loss\" =  111.89674\n",
      "2018-09-02 23:36:13.969908 Test Step 1055 \"loss\" =  112.059906\n",
      "2018-09-02 23:36:17.243361 Training Step 1055 Finished Timing (Training: 0.917969, Test: 0.0819931) after 17.0151 seconds\n",
      "2018-09-02 23:36:17.243445 Training Step 1055 \"min loss\" =  18.986729\n",
      "2018-09-02 23:36:17.243487 Training Step 1055 \"loss\" =  19.815907\n",
      "2018-09-02 23:36:30.830029 Test Step 1060 Finished\n",
      "2018-09-02 23:36:30.830135 Test Step 1060 \"min loss\" =  111.89674\n",
      "2018-09-02 23:36:30.830189 Test Step 1060 \"loss\" =  113.09087\n",
      "2018-09-02 23:36:33.962601 Training Step 1060 Finished Timing (Training: 0.917876, Test: 0.082087) after 16.7191 seconds\n",
      "2018-09-02 23:36:33.962689 Training Step 1060 \"min loss\" =  18.986729\n",
      "2018-09-02 23:36:33.962731 Training Step 1060 \"loss\" =  19.86857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-09-02 23:36:47.760231 Test Step 1065 Finished\n",
      "2018-09-02 23:36:47.760325 Test Step 1065 \"min loss\" =  111.89674\n",
      "2018-09-02 23:36:47.760617 Test Step 1065 \"loss\" =  113.51517\n",
      "2018-09-02 23:36:50.736621 Training Step 1065 Finished Timing (Training: 0.917484, Test: 0.0824794) after 16.7738 seconds\n",
      "2018-09-02 23:36:50.736951 Training Step 1065 \"min loss\" =  18.523376\n",
      "2018-09-02 23:36:50.737038 Training Step 1065 \"loss\" =  20.742077\n",
      "2018-09-02 23:37:04.260971 Test Step 1070 Finished\n",
      "2018-09-02 23:37:04.261080 Test Step 1070 \"min loss\" =  111.89674\n",
      "2018-09-02 23:37:04.261392 Test Step 1070 \"loss\" =  114.43892\n",
      "2018-09-02 23:37:07.269660 Training Step 1070 Finished Timing (Training: 0.917419, Test: 0.0825425) after 16.5325 seconds\n",
      "2018-09-02 23:37:07.269751 Training Step 1070 \"min loss\" =  18.523376\n",
      "2018-09-02 23:37:07.269793 Training Step 1070 \"loss\" =  19.427544\n",
      "2018-09-02 23:37:20.781202 Test Step 1075 Finished\n",
      "2018-09-02 23:37:20.781620 Test Step 1075 \"min loss\" =  111.89674\n",
      "2018-09-02 23:37:20.781677 Test Step 1075 \"loss\" =  114.18421\n",
      "2018-09-02 23:37:23.729227 Training Step 1075 Finished Timing (Training: 0.917208, Test: 0.0827527) after 16.4594 seconds\n",
      "2018-09-02 23:37:23.729315 Training Step 1075 \"min loss\" =  18.523376\n",
      "2018-09-02 23:37:23.729357 Training Step 1075 \"loss\" =  19.72437\n",
      "2018-09-02 23:37:37.100702 Test Step 1080 Finished\n",
      "2018-09-02 23:37:37.100980 Test Step 1080 \"min loss\" =  111.89674\n",
      "2018-09-02 23:37:37.101041 Test Step 1080 \"loss\" =  113.82472\n",
      "2018-09-02 23:37:40.019461 Training Step 1080 Finished Timing (Training: 0.916987, Test: 0.0829742) after 16.2901 seconds\n",
      "2018-09-02 23:37:40.019568 Training Step 1080 \"min loss\" =  18.523376\n",
      "2018-09-02 23:37:40.019609 Training Step 1080 \"loss\" =  20.375257\n",
      "2018-09-02 23:37:53.327974 Test Step 1085 Finished\n",
      "2018-09-02 23:37:53.328075 Test Step 1085 \"min loss\" =  111.89674\n",
      "2018-09-02 23:37:53.328129 Test Step 1085 \"loss\" =  112.82466\n",
      "2018-09-02 23:37:56.132760 Training Step 1085 Finished Timing (Training: 0.916896, Test: 0.0830662) after 16.1131 seconds\n",
      "2018-09-02 23:37:56.132857 Training Step 1085 \"min loss\" =  18.468512\n",
      "2018-09-02 23:37:56.132899 Training Step 1085 \"loss\" =  19.57594\n",
      "2018-09-02 23:38:09.400951 Test Step 1090 Finished\n",
      "2018-09-02 23:38:09.401050 Test Step 1090 \"min loss\" =  111.89674\n",
      "2018-09-02 23:38:09.401331 Test Step 1090 \"loss\" =  113.88995\n",
      "2018-09-02 23:38:12.211160 Training Step 1090 Finished Timing (Training: 0.91662, Test: 0.0833422) after 16.0782 seconds\n",
      "2018-09-02 23:38:12.211482 Training Step 1090 \"min loss\" =  18.468512\n",
      "2018-09-02 23:38:12.211545 Training Step 1090 \"loss\" =  18.55558\n",
      "2018-09-02 23:38:25.648791 Test Step 1095 Finished\n",
      "2018-09-02 23:38:25.648924 Test Step 1095 \"min loss\" =  111.89674\n",
      "2018-09-02 23:38:25.649010 Test Step 1095 \"loss\" =  113.448456\n",
      "2018-09-02 23:38:28.362506 Training Step 1095 Finished Timing (Training: 0.916542, Test: 0.0834189) after 16.1509 seconds\n",
      "2018-09-02 23:38:28.362593 Training Step 1095 \"min loss\" =  18.468512\n",
      "2018-09-02 23:38:28.362630 Training Step 1095 \"loss\" =  21.025017\n",
      "2018-09-02 23:38:40.398117 Test Step 1100 Finished\n",
      "2018-09-02 23:38:40.398217 Test Step 1100 \"min loss\" =  111.89674\n",
      "2018-09-02 23:38:40.398262 Test Step 1100 \"loss\" =  112.70726\n",
      "2018-09-02 23:38:43.078419 Training Step 1100 Finished Timing (Training: 0.916513, Test: 0.0834475) after 14.7152 seconds\n",
      "2018-09-02 23:38:43.078512 Training Step 1100 \"min loss\" =  18.468512\n",
      "2018-09-02 23:38:43.078564 Training Step 1100 \"loss\" =  20.779379\n",
      "2018-09-02 23:38:55.212132 Test Step 1105 Finished\n",
      "2018-09-02 23:38:55.212236 Test Step 1105 \"min loss\" =  111.89674\n",
      "2018-09-02 23:38:55.212507 Test Step 1105 \"loss\" =  112.40737\n",
      "2018-09-02 23:38:57.999274 Training Step 1105 Finished Timing (Training: 0.916883, Test: 0.0830861) after 14.9207 seconds\n",
      "2018-09-02 23:38:57.999362 Training Step 1105 \"min loss\" =  18.468512\n",
      "2018-09-02 23:38:57.999405 Training Step 1105 \"loss\" =  19.720894\n",
      "2018-09-02 23:39:09.735491 Test Step 1110 Finished\n",
      "2018-09-02 23:39:09.735585 Test Step 1110 \"min loss\" =  111.89674\n",
      "2018-09-02 23:39:09.735629 Test Step 1110 \"loss\" =  113.61937\n",
      "2018-09-02 23:39:12.578826 Training Step 1110 Finished Timing (Training: 0.918189, Test: 0.081783) after 14.5794 seconds\n",
      "2018-09-02 23:39:12.578926 Training Step 1110 \"min loss\" =  18.468512\n",
      "2018-09-02 23:39:12.578973 Training Step 1110 \"loss\" =  21.57839\n",
      "2018-09-02 23:39:24.376184 Test Step 1115 Finished\n",
      "2018-09-02 23:39:24.376276 Test Step 1115 \"min loss\" =  111.89674\n",
      "2018-09-02 23:39:24.376321 Test Step 1115 \"loss\" =  112.930305\n",
      "2018-09-02 23:39:27.184194 Training Step 1115 Finished Timing (Training: 0.919411, Test: 0.080561) after 14.6052 seconds\n",
      "2018-09-02 23:39:27.184293 Training Step 1115 \"min loss\" =  18.468512\n",
      "2018-09-02 23:39:27.184340 Training Step 1115 \"loss\" =  19.862736\n",
      "2018-09-02 23:39:39.390598 Test Step 1120 Finished\n",
      "2018-09-02 23:39:39.390688 Test Step 1120 \"min loss\" =  111.89674\n",
      "2018-09-02 23:39:39.390725 Test Step 1120 \"loss\" =  112.47266\n",
      "2018-09-02 23:39:42.082504 Training Step 1120 Finished Timing (Training: 0.919766, Test: 0.0802068) after 14.8981 seconds\n",
      "2018-09-02 23:39:42.082600 Training Step 1120 \"min loss\" =  17.3217\n",
      "2018-09-02 23:39:42.082641 Training Step 1120 \"loss\" =  17.3217\n",
      "2018-09-02 23:39:54.787574 Test Step 1125 Finished\n",
      "2018-09-02 23:39:54.787664 Test Step 1125 \"min loss\" =  111.89674\n",
      "2018-09-02 23:39:54.787702 Test Step 1125 \"loss\" =  112.937454\n",
      "2018-09-02 23:39:57.419780 Training Step 1125 Finished Timing (Training: 0.920763, Test: 0.0792098) after 15.3371 seconds\n",
      "2018-09-02 23:39:57.419879 Training Step 1125 \"min loss\" =  17.304148\n",
      "2018-09-02 23:39:57.419932 Training Step 1125 \"loss\" =  17.304148\n",
      "2018-09-02 23:40:09.849588 Test Step 1130 Finished\n",
      "2018-09-02 23:40:09.849702 Test Step 1130 \"min loss\" =  111.89674\n",
      "2018-09-02 23:40:09.849760 Test Step 1130 \"loss\" =  112.93721\n",
      "2018-09-02 23:40:12.472300 Training Step 1130 Finished Timing (Training: 0.920244, Test: 0.0797289) after 15.0523 seconds\n",
      "2018-09-02 23:40:12.472392 Training Step 1130 \"min loss\" =  16.5244\n",
      "2018-09-02 23:40:12.472434 Training Step 1130 \"loss\" =  19.346972\n",
      "2018-09-02 23:40:24.935806 Test Step 1135 Finished\n",
      "2018-09-02 23:40:24.936073 Test Step 1135 \"min loss\" =  111.89674\n",
      "2018-09-02 23:40:24.936115 Test Step 1135 \"loss\" =  113.418945\n",
      "2018-09-02 23:40:27.605241 Training Step 1135 Finished Timing (Training: 0.91934, Test: 0.0806315) after 15.1328 seconds\n",
      "2018-09-02 23:40:27.605331 Training Step 1135 \"min loss\" =  16.5244\n",
      "2018-09-02 23:40:27.605388 Training Step 1135 \"loss\" =  19.430696\n",
      "2018-09-02 23:40:39.821817 Test Step 1140 Finished\n",
      "2018-09-02 23:40:39.821921 Test Step 1140 \"min loss\" =  111.89674\n",
      "2018-09-02 23:40:39.821968 Test Step 1140 \"loss\" =  113.4062\n",
      "2018-09-02 23:40:42.578307 Training Step 1140 Finished Timing (Training: 0.91886, Test: 0.0811111) after 14.9729 seconds\n",
      "2018-09-02 23:40:42.578397 Training Step 1140 \"min loss\" =  16.5244\n",
      "2018-09-02 23:40:42.578439 Training Step 1140 \"loss\" =  18.815699\n",
      "2018-09-02 23:40:55.351076 Test Step 1145 Finished\n",
      "2018-09-02 23:40:55.351193 Test Step 1145 \"min loss\" =  111.89674\n",
      "2018-09-02 23:40:55.351259 Test Step 1145 \"loss\" =  113.69324\n",
      "2018-09-02 23:40:58.189610 Training Step 1145 Finished Timing (Training: 0.918755, Test: 0.0812162) after 15.6111 seconds\n",
      "2018-09-02 23:40:58.189713 Training Step 1145 \"min loss\" =  16.5244\n",
      "2018-09-02 23:40:58.189768 Training Step 1145 \"loss\" =  19.467655\n",
      "2018-09-02 23:41:10.597247 Test Step 1150 Finished\n",
      "2018-09-02 23:41:10.597356 Test Step 1150 \"min loss\" =  111.89674\n",
      "2018-09-02 23:41:10.597405 Test Step 1150 \"loss\" =  112.23444\n",
      "2018-09-02 23:41:13.561134 Training Step 1150 Finished Timing (Training: 0.918692, Test: 0.0812791) after 15.3713 seconds\n",
      "2018-09-02 23:41:13.561269 Training Step 1150 \"min loss\" =  16.5244\n",
      "2018-09-02 23:41:13.561350 Training Step 1150 \"loss\" =  19.065615\n",
      "2018-09-02 23:41:25.512570 Test Step 1155 Finished\n",
      "2018-09-02 23:41:25.512696 Test Step 1155 \"min loss\" =  111.89674\n",
      "2018-09-02 23:41:25.512765 Test Step 1155 \"loss\" =  113.645134\n",
      "2018-09-02 23:41:28.917223 Training Step 1155 Finished Timing (Training: 0.919205, Test: 0.0807653) after 15.3558 seconds\n",
      "2018-09-02 23:41:28.917330 Training Step 1155 \"min loss\" =  16.5244\n",
      "2018-09-02 23:41:28.917403 Training Step 1155 \"loss\" =  16.747875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-09-02 23:41:41.787932 Test Step 1160 Finished\n",
      "2018-09-02 23:41:41.788064 Test Step 1160 \"min loss\" =  111.89674\n",
      "2018-09-02 23:41:41.788137 Test Step 1160 \"loss\" =  113.36212\n",
      "2018-09-02 23:41:44.439708 Training Step 1160 Finished Timing (Training: 0.919082, Test: 0.0808872) after 15.5222 seconds\n",
      "2018-09-02 23:41:44.439800 Training Step 1160 \"min loss\" =  16.5244\n",
      "2018-09-02 23:41:44.439843 Training Step 1160 \"loss\" =  20.777351\n",
      "2018-09-02 23:41:56.843752 Test Step 1165 Finished\n",
      "2018-09-02 23:41:56.843846 Test Step 1165 \"min loss\" =  111.89674\n",
      "2018-09-02 23:41:56.843886 Test Step 1165 \"loss\" =  114.10826\n",
      "2018-09-02 23:41:59.594815 Training Step 1165 Finished Timing (Training: 0.918733, Test: 0.0812368) after 15.1549 seconds\n",
      "2018-09-02 23:41:59.594915 Training Step 1165 \"min loss\" =  16.5244\n",
      "2018-09-02 23:41:59.594960 Training Step 1165 \"loss\" =  17.104036\n",
      "2018-09-02 23:42:12.148874 Test Step 1170 Finished\n",
      "2018-09-02 23:42:12.148994 Test Step 1170 \"min loss\" =  111.89674\n",
      "2018-09-02 23:42:12.149067 Test Step 1170 \"loss\" =  113.221924\n",
      "2018-09-02 23:42:14.954211 Training Step 1170 Finished Timing (Training: 0.918516, Test: 0.0814538) after 15.3592 seconds\n",
      "2018-09-02 23:42:14.954313 Training Step 1170 \"min loss\" =  16.5244\n",
      "2018-09-02 23:42:14.954369 Training Step 1170 \"loss\" =  18.419033\n",
      "2018-09-02 23:42:27.033427 Test Step 1175 Finished\n",
      "2018-09-02 23:42:27.033535 Test Step 1175 \"min loss\" =  111.89674\n",
      "2018-09-02 23:42:27.033582 Test Step 1175 \"loss\" =  112.18193\n",
      "2018-09-02 23:42:30.239592 Training Step 1175 Finished Timing (Training: 0.917554, Test: 0.0824161) after 15.2852 seconds\n",
      "2018-09-02 23:42:30.239681 Training Step 1175 \"min loss\" =  16.5244\n",
      "2018-09-02 23:42:30.239722 Training Step 1175 \"loss\" =  18.055107\n",
      "2018-09-02 23:42:42.719677 Test Step 1180 Finished\n",
      "2018-09-02 23:42:42.719779 Test Step 1180 \"min loss\" =  111.37962\n",
      "2018-09-02 23:42:42.719826 Test Step 1180 \"loss\" =  111.37962\n",
      "2018-09-02 23:42:45.792355 Training Step 1180 Finished Timing (Training: 0.917432, Test: 0.0825378) after 15.5526 seconds\n",
      "2018-09-02 23:42:45.792478 Training Step 1180 \"min loss\" =  16.5244\n",
      "2018-09-02 23:42:45.792545 Training Step 1180 \"loss\" =  19.617872\n",
      "2018-09-02 23:42:59.494339 Test Step 1185 Finished\n",
      "2018-09-02 23:42:59.494425 Test Step 1185 \"min loss\" =  111.37962\n",
      "2018-09-02 23:42:59.494473 Test Step 1185 \"loss\" =  112.16982\n",
      "2018-09-02 23:43:02.479949 Training Step 1185 Finished Timing (Training: 0.917852, Test: 0.0821189) after 16.6873 seconds\n",
      "2018-09-02 23:43:02.480283 Training Step 1185 \"min loss\" =  16.5244\n",
      "2018-09-02 23:43:02.480329 Training Step 1185 \"loss\" =  17.050545\n",
      "2018-09-02 23:43:16.172074 Test Step 1190 Finished\n",
      "2018-09-02 23:43:16.172201 Test Step 1190 \"min loss\" =  111.37962\n",
      "2018-09-02 23:43:16.172278 Test Step 1190 \"loss\" =  114.033424\n",
      "2018-09-02 23:43:19.303396 Training Step 1190 Finished Timing (Training: 0.918121, Test: 0.0818488) after 16.823 seconds\n",
      "2018-09-02 23:43:19.303491 Training Step 1190 \"min loss\" =  16.5244\n",
      "2018-09-02 23:43:19.303538 Training Step 1190 \"loss\" =  19.179083\n",
      "2018-09-02 23:43:32.903746 Test Step 1195 Finished\n",
      "2018-09-02 23:43:32.903840 Test Step 1195 \"min loss\" =  111.37962\n",
      "2018-09-02 23:43:32.903881 Test Step 1195 \"loss\" =  112.89084\n",
      "2018-09-02 23:43:36.024487 Training Step 1195 Finished Timing (Training: 0.918455, Test: 0.0815147) after 16.7209 seconds\n",
      "2018-09-02 23:43:36.024761 Training Step 1195 \"min loss\" =  16.5244\n",
      "2018-09-02 23:43:36.024806 Training Step 1195 \"loss\" =  18.213442\n",
      "2018-09-02 23:43:49.529424 Test Step 1200 Finished\n",
      "2018-09-02 23:43:49.529526 Test Step 1200 \"min loss\" =  111.37962\n",
      "2018-09-02 23:43:49.529582 Test Step 1200 \"loss\" =  113.308876\n",
      "2018-09-02 23:43:52.724531 Training Step 1200 Finished Timing (Training: 0.918595, Test: 0.0813746) after 16.6997 seconds\n",
      "2018-09-02 23:43:52.724650 Training Step 1200 \"min loss\" =  16.5244\n",
      "2018-09-02 23:43:52.724716 Training Step 1200 \"loss\" =  19.69913\n",
      "2018-09-02 23:44:06.241164 Test Step 1205 Finished\n",
      "2018-09-02 23:44:06.241265 Test Step 1205 \"min loss\" =  111.37962\n",
      "2018-09-02 23:44:06.241312 Test Step 1205 \"loss\" =  113.852936\n",
      "2018-09-02 23:44:09.477334 Training Step 1205 Finished Timing (Training: 0.921251, Test: 0.0787362) after 16.7525 seconds\n",
      "2018-09-02 23:44:09.477479 Training Step 1205 \"min loss\" =  16.5244\n",
      "2018-09-02 23:44:09.477570 Training Step 1205 \"loss\" =  17.48876\n",
      "2018-09-02 23:44:22.966987 Test Step 1210 Finished\n",
      "2018-09-02 23:44:22.967100 Test Step 1210 \"min loss\" =  111.37962\n",
      "2018-09-02 23:44:22.967158 Test Step 1210 \"loss\" =  114.03066\n",
      "2018-09-02 23:44:26.503868 Training Step 1210 Finished Timing (Training: 0.913461, Test: 0.0865153) after 17.0262 seconds\n",
      "2018-09-02 23:44:26.503966 Training Step 1210 \"min loss\" =  16.5244\n",
      "2018-09-02 23:44:26.504014 Training Step 1210 \"loss\" =  17.700766\n",
      "2018-09-02 23:44:40.207819 Test Step 1215 Finished\n",
      "2018-09-02 23:44:40.207948 Test Step 1215 \"min loss\" =  111.37962\n",
      "2018-09-02 23:44:40.208013 Test Step 1215 \"loss\" =  115.601685\n",
      "2018-09-02 23:44:43.094031 Training Step 1215 Finished Timing (Training: 0.917921, Test: 0.0820538) after 16.59 seconds\n",
      "2018-09-02 23:44:43.094148 Training Step 1215 \"min loss\" =  16.5244\n",
      "2018-09-02 23:44:43.094201 Training Step 1215 \"loss\" =  18.383274\n",
      "2018-09-02 23:44:55.743601 Test Step 1220 Finished\n",
      "2018-09-02 23:44:55.743704 Test Step 1220 \"min loss\" =  111.37962\n",
      "2018-09-02 23:44:55.743745 Test Step 1220 \"loss\" =  113.442024\n",
      "2018-09-02 23:44:58.367556 Training Step 1220 Finished Timing (Training: 0.919226, Test: 0.0807474) after 15.2733 seconds\n",
      "2018-09-02 23:44:58.367652 Training Step 1220 \"min loss\" =  15.871497\n",
      "2018-09-02 23:44:58.367697 Training Step 1220 \"loss\" =  18.735842\n",
      "2018-09-02 23:45:10.995288 Test Step 1225 Finished\n",
      "2018-09-02 23:45:10.995396 Test Step 1225 \"min loss\" =  111.37962\n",
      "2018-09-02 23:45:10.995445 Test Step 1225 \"loss\" =  113.38638\n",
      "2018-09-02 23:45:13.907476 Training Step 1225 Finished Timing (Training: 0.918768, Test: 0.081206) after 15.5397 seconds\n",
      "2018-09-02 23:45:13.907566 Training Step 1225 \"min loss\" =  15.871497\n",
      "2018-09-02 23:45:13.907608 Training Step 1225 \"loss\" =  18.371227\n",
      "2018-09-02 23:45:26.053290 Test Step 1230 Finished\n",
      "2018-09-02 23:45:26.053595 Test Step 1230 \"min loss\" =  111.37962\n",
      "2018-09-02 23:45:26.053641 Test Step 1230 \"loss\" =  112.78234\n",
      "2018-09-02 23:45:29.428796 Training Step 1230 Finished Timing (Training: 0.918695, Test: 0.0812765) after 15.5211 seconds\n",
      "2018-09-02 23:45:29.428891 Training Step 1230 \"min loss\" =  15.871497\n",
      "2018-09-02 23:45:29.428934 Training Step 1230 \"loss\" =  19.097725\n",
      "2018-09-02 23:45:42.845964 Test Step 1235 Finished\n",
      "2018-09-02 23:45:42.846077 Test Step 1235 \"min loss\" =  111.37962\n",
      "2018-09-02 23:45:42.846160 Test Step 1235 \"loss\" =  112.793106\n",
      "2018-09-02 23:45:46.472489 Training Step 1235 Finished Timing (Training: 0.917734, Test: 0.0822374) after 17.0435 seconds\n",
      "2018-09-02 23:45:46.472591 Training Step 1235 \"min loss\" =  15.871497\n",
      "2018-09-02 23:45:46.472644 Training Step 1235 \"loss\" =  17.106653\n",
      "2018-09-02 23:46:00.889602 Test Step 1240 Finished\n",
      "2018-09-02 23:46:00.889726 Test Step 1240 \"min loss\" =  111.37962\n",
      "2018-09-02 23:46:00.889803 Test Step 1240 \"loss\" =  113.07141\n",
      "2018-09-02 23:46:04.262156 Training Step 1240 Finished Timing (Training: 0.915868, Test: 0.0841037) after 17.7895 seconds\n",
      "2018-09-02 23:46:04.262255 Training Step 1240 \"min loss\" =  15.871497\n",
      "2018-09-02 23:46:04.262574 Training Step 1240 \"loss\" =  17.652803\n",
      "2018-09-02 23:46:18.013598 Test Step 1245 Finished\n",
      "2018-09-02 23:46:18.013709 Test Step 1245 \"min loss\" =  111.37962\n",
      "2018-09-02 23:46:18.013775 Test Step 1245 \"loss\" =  113.08592\n",
      "2018-09-02 23:46:21.454178 Training Step 1245 Finished Timing (Training: 0.916591, Test: 0.0833789) after 17.1915 seconds\n",
      "2018-09-02 23:46:21.454502 Training Step 1245 \"min loss\" =  15.871497\n",
      "2018-09-02 23:46:21.454579 Training Step 1245 \"loss\" =  18.375362\n",
      "2018-09-02 23:46:35.222337 Test Step 1250 Finished\n",
      "2018-09-02 23:46:35.222440 Test Step 1250 \"min loss\" =  111.37962\n",
      "2018-09-02 23:46:35.222487 Test Step 1250 \"loss\" =  112.88535\n",
      "2018-09-02 23:46:38.541745 Training Step 1250 Finished Timing (Training: 0.916467, Test: 0.0835013) after 17.0871 seconds\n",
      "2018-09-02 23:46:38.541865 Training Step 1250 \"min loss\" =  15.871497\n",
      "2018-09-02 23:46:38.541936 Training Step 1250 \"loss\" =  18.146944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-09-02 23:46:51.708333 Test Step 1255 Finished\n",
      "2018-09-02 23:46:51.708628 Test Step 1255 \"min loss\" =  111.37962\n",
      "2018-09-02 23:46:51.708673 Test Step 1255 \"loss\" =  111.99585\n",
      "2018-09-02 23:46:54.583630 Training Step 1255 Finished Timing (Training: 0.91682, Test: 0.0831479) after 16.0416 seconds\n",
      "2018-09-02 23:46:54.583720 Training Step 1255 \"min loss\" =  15.871497\n",
      "2018-09-02 23:46:54.583762 Training Step 1255 \"loss\" =  19.111843\n",
      "2018-09-02 23:47:07.404482 Test Step 1260 Finished\n",
      "2018-09-02 23:47:07.404588 Test Step 1260 \"min loss\" =  111.37962\n",
      "2018-09-02 23:47:07.404650 Test Step 1260 \"loss\" =  113.05188\n",
      "2018-09-02 23:47:10.373363 Training Step 1260 Finished Timing (Training: 0.916486, Test: 0.0834822) after 15.7896 seconds\n",
      "2018-09-02 23:47:10.373460 Training Step 1260 \"min loss\" =  15.871497\n",
      "2018-09-02 23:47:10.373506 Training Step 1260 \"loss\" =  18.809568\n",
      "2018-09-02 23:47:22.204406 Test Step 1265 Finished\n",
      "2018-09-02 23:47:22.204501 Test Step 1265 \"min loss\" =  111.37962\n",
      "2018-09-02 23:47:22.204557 Test Step 1265 \"loss\" =  112.18431\n",
      "2018-09-02 23:47:24.839388 Training Step 1265 Finished Timing (Training: 0.916938, Test: 0.0830299) after 14.4658 seconds\n",
      "2018-09-02 23:47:24.839478 Training Step 1265 \"min loss\" =  15.871497\n",
      "2018-09-02 23:47:24.839520 Training Step 1265 \"loss\" =  19.275066\n",
      "2018-09-02 23:47:37.331297 Test Step 1270 Finished\n",
      "2018-09-02 23:47:37.331391 Test Step 1270 \"min loss\" =  111.37962\n",
      "2018-09-02 23:47:37.331433 Test Step 1270 \"loss\" =  112.097595\n",
      "2018-09-02 23:47:40.043594 Training Step 1270 Finished Timing (Training: 0.916927, Test: 0.0830414) after 15.204 seconds\n",
      "2018-09-02 23:47:40.043685 Training Step 1270 \"min loss\" =  15.871497\n",
      "2018-09-02 23:47:40.043727 Training Step 1270 \"loss\" =  17.351631\n",
      "2018-09-02 23:47:52.408537 Test Step 1275 Finished\n",
      "2018-09-02 23:47:52.408647 Test Step 1275 \"min loss\" =  111.37962\n",
      "2018-09-02 23:47:52.408708 Test Step 1275 \"loss\" =  111.9814\n",
      "2018-09-02 23:47:55.049977 Training Step 1275 Finished Timing (Training: 0.917343, Test: 0.0826259) after 15.0062 seconds\n",
      "2018-09-02 23:47:55.050067 Training Step 1275 \"min loss\" =  15.871497\n",
      "2018-09-02 23:47:55.050124 Training Step 1275 \"loss\" =  17.905935\n",
      "2018-09-02 23:48:09.078353 Test Step 1280 Finished\n",
      "2018-09-02 23:48:09.078439 Test Step 1280 \"min loss\" =  111.37962\n",
      "2018-09-02 23:48:09.078486 Test Step 1280 \"loss\" =  112.18421\n",
      "2018-09-02 23:48:11.948194 Training Step 1280 Finished Timing (Training: 0.91821, Test: 0.0817595) after 16.898 seconds\n",
      "2018-09-02 23:48:11.948304 Training Step 1280 \"min loss\" =  15.871497\n",
      "2018-09-02 23:48:11.948366 Training Step 1280 \"loss\" =  17.103098\n",
      "2018-09-02 23:48:24.674186 Test Step 1285 Finished\n",
      "2018-09-02 23:48:24.674285 Test Step 1285 \"min loss\" =  111.37962\n",
      "2018-09-02 23:48:24.674327 Test Step 1285 \"loss\" =  111.8959\n",
      "2018-09-02 23:48:27.457690 Training Step 1285 Finished Timing (Training: 0.918586, Test: 0.0813838) after 15.5093 seconds\n",
      "2018-09-02 23:48:27.457788 Training Step 1285 \"min loss\" =  15.449285\n",
      "2018-09-02 23:48:27.457836 Training Step 1285 \"loss\" =  15.449285\n",
      "2018-09-02 23:48:40.398669 Test Step 1290 Finished\n",
      "2018-09-02 23:48:40.398756 Test Step 1290 \"min loss\" =  111.37962\n",
      "2018-09-02 23:48:40.398793 Test Step 1290 \"loss\" =  112.27901\n",
      "2018-09-02 23:48:43.266678 Training interrupted at 1290\n",
      "2018-09-02 23:48:43.266777 Training completed, starting cleanup!\n",
      "2018-09-02 23:48:43.266845 Cleanup completed!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8594f59d515f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                  \"linkage_adjustment_components\": 3}\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msupervised\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpParameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpParameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-cd974459e9cf>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(no_folds, supervised, i, l, n, expParameters)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;31m# Node type of input nodes: 0 = training set; 1 = test set; -1 = neither\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/Study/2. COMP5329 - Deep Learning/Assessment/Assignment 2/Paper/GGCNN/src/ggcnn/experiment.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mwasKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mraisedEx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/Study/2. COMP5329 - Deep Learning/Assessment/Assignment 2/Paper/GGCNN/src/ggcnn/experiment.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m                     \u001b[0mstart_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                     \u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreports\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msummary_merged\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreports\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m                     \u001b[0mall_training_reports\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreports\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                     \u001b[0mtotal_training\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_temp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "expParameters = {\"reverseLinkagePosition\": \"Early\", \"linkagePosition\": \"Late\", \"linkageActFun\": False, \"linkageBatchNorm\": True,\n",
    "                 \"linkageNeurons\": None, \"auxilaryEmbedding1\": False, \"auxilaryEmbedding2\": False, \"auxilaryGraph\": True,\n",
    "                 \"linkage_adjustment_components\": 3}\n",
    "\n",
    "results, idx_split = run(supervised = False, expParameters = expParameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(results[0])\n",
    "test_df = pd.DataFrame(results[1])\n",
    "test_df.set_index(test_df.index * 5, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_df['accuracy'].plot()\n",
    "test_df['accuracy'].plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df['cross_entropy'].loc[10:].plot()\n",
    "test_df['cross_entropy'].loc[10:].plot()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.008738040924072\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_df['loss'].loc[500:].plot()\n",
    "test_df['loss'].loc[500:].plot()\n",
    "train_df['min loss'].loc[500:].plot()\n",
    "test_df['min loss'].loc[500:].plot()\n",
    "print(test_df['min loss'].iloc[-1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = np.flatnonzero(idx_split == 1)\n",
    "\n",
    "predictions = results[-1].ravel()[test_idx]\n",
    "actual = dataset[2].ravel()[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max error:  23.091601316227496\n"
     ]
    }
   ],
   "source": [
    "plt.scatter(actual, predictions); plt.xlabel('Actual'); plt.ylabel('Predicted'); plt.show()\n",
    "print('Max error: ', np.max(np.abs(actual - predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAE0lJREFUeJzt3X+MZeV93/H3pxBInSgBzODg3aVDmk0abLkymmDa9Ac1MT8tr1sFCRLVWwdplQQnTp3KLOEPqkSW1kplYjcO0tZsAYlCkOOEVcElG2yXVgqYxXEwsHYYYcqOIey4i0laFNO1v/3jPpu9nr2zMzt3du7uPu+XNLrnfM9z73nu2dn7meecc89JVSFJ6s/fmXQHJEmTYQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOnXqUg2S7ADeDeyrqrcO1X8F+ABwAHigqj7c6jcB1wPfAX61qh5q9SuAjwOnAJ+qqm1Lrfvss8+u6enpo31PktS1J5544ptVNbVUuyUDALgD+F3groOFJP8C2AS8raq+neScVr8AuBZ4C/Bm4E+S/Hh72ieBdwFzwONJdlbVM0da8fT0NLt3715GFyVJByX5X8tpt2QAVNUjSaYXlH8J2FZV325t9rX6JuDeVv96klngorZstqqea527t7U9YgBIko6dlR4D+HHgnyZ5LMl/T/JTrb4O2DvUbq7VFqtLkiZkObuAFnvemcDFwE8B9yX5USAj2hajg2bkZUiTbAG2AJx33nkr7J4kaSkrHQHMAZ+pgS8C3wXObvUNQ+3WAy8eoX6YqtpeVTNVNTM1teQxDEnSCq00AP4IeCdAO8h7GvBNYCdwbZLTk5wPbAS+CDwObExyfpLTGBwo3jlu5yVJK7ec00DvAS4Bzk4yB9wC7AB2JHkKeB3YXIM7yzyd5D4GB3cPADdU1Xfa63wAeIjBaaA7qurpY/B+JEnLlOP5jmAzMzPlaaCSdHSSPFFVM0u185vAktQpA0CSOrXS00Cl7kxvfeB75p/fdvWEeiKtDkcAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVOeBiqtkKeF6kTnCECSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUqSUDIMmOJPva/X8XLvt3SSrJ2W0+ST6RZDbJk0kuHGq7Ocmz7Wfz6r4NSdLRWs4I4A7gioXFJBuAdwEvDJWvBDa2ny3Aba3tWQxuJv8O4CLgliRnjtNxSdJ4lgyAqnoE2D9i0a3Ah4Hhu8pvAu6qgUeBM5KcC1wO7Kqq/VX1CrCLEaEiSVo7KzoGkOQ9wDeq6s8XLFoH7B2an2u1xeqSpAk56ovBJXkDcDNw2ajFI2p1hPqo19/CYPcR55133tF2T5K0TCsZAfx94Hzgz5M8D6wHvpTkRxj8Zb9hqO164MUj1A9TVduraqaqZqamplbQPUnSchx1AFTVV6rqnKqarqppBh/uF1bVXwI7gfe1s4EuBl6tqpeAh4DLkpzZDv5e1mqSpAlZzmmg9wB/CvxEkrkk1x+h+YPAc8As8J+AXwaoqv3AbwGPt5/fbDVJ0oQseQygqq5bYvn00HQBNyzSbgew4yj7J0k6RvwmsCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkTi3nnsA7kuxL8tRQ7beTfDXJk0n+MMkZQ8tuSjKb5GtJLh+qX9Fqs0m2rv5bkSQdjeWMAO4ArlhQ2wW8tareBvwFcBNAkguAa4G3tOf8XpJTkpwCfBK4ErgAuK61lSRNyJIBUFWPAPsX1P64qg602UeB9W16E3BvVX27qr4OzAIXtZ/Zqnquql4H7m1tJUkTshrHAH4B+GybXgfsHVo212qL1Q+TZEuS3Ul2z8/Pr0L3JEmjnDrOk5PcDBwA7j5YGtGsGB00Neo1q2o7sB1gZmZmZBvpeDS99YG/nX5+29UT7Im0PCsOgCSbgXcDl1bVwQ/qOWDDULP1wItterG6JGkCVhQASa4AbgT+eVW9NrRoJ/BfknwMeDOwEfgig5HBxiTnA99gcKD458bpuHSsDf9FL52MlgyAJPcAlwBnJ5kDbmFw1s/pwK4kAI9W1S9W1dNJ7gOeYbBr6Iaq+k57nQ8ADwGnADuq6ulj8H4kScu0ZABU1XUjyrcfof1HgI+MqD8IPHhUvZMkHTN+E1iSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqeWDIAkO5LsS/LUUO2sJLuSPNsez2z1JPlEktkkTya5cOg5m1v7Z5NsPjZvR5K0XMsZAdwBXLGgthV4uKo2Ag+3eYArgY3tZwtwGwwCg8HN5N8BXATccjA0JEmTsWQAVNUjwP4F5U3AnW36TuC9Q/W7auBR4Iwk5wKXA7uqan9VvQLs4vBQkSStoZUeA3hTVb0E0B7PafV1wN6hdnOttlj9MEm2JNmdZPf8/PwKuydJWspqHwTOiFodoX54sWp7Vc1U1czU1NSqdk6SdMhKA+DltmuH9riv1eeADUPt1gMvHqEuSZqQlQbATuDgmTybgfuH6u9rZwNdDLzadhE9BFyW5Mx28PeyVpMkTcipSzVIcg9wCXB2kjkGZ/NsA+5Lcj3wAnBNa/4gcBUwC7wGvB+gqvYn+S3g8dbuN6tq4YFlSdIaWjIAquq6RRZdOqJtATcs8jo7gB1H1TtJ0jHjN4ElqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASerUkt8DkHoxvfWBSXdBWlOOACSpUwaAJHXKAJCkTnkMQDoGFh5PeH7b1RPqibQ4RwCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0aKwCS/NskTyd5Ksk9Sb4/yflJHkvybJLfT3Jaa3t6m59ty6dX4w1IklZmxQGQZB3wq8BMVb0VOAW4FvgocGtVbQReAa5vT7keeKWqfgy4tbWTJE3IuLuATgX+bpJTgTcALwHvBD7dlt8JvLdNb2rztOWXJsmY65ckrdCKA6CqvgH8B+AFBh/8rwJPAN+qqgOt2Rywrk2vA/a25x5o7d+40vVLksYzzi6gMxn8VX8+8GbgB4ArRzStg085wrLh192SZHeS3fPz8yvtniRpCePsAvoZ4OtVNV9V/w/4DPCPgTPaLiGA9cCLbXoO2ADQlv8wsH/hi1bV9qqaqaqZqampMbonSTqScQLgBeDiJG9o+/IvBZ4BPg/8bGuzGbi/Te9s87Tln6uqw0YAkqS1Mc4xgMcYHMz9EvCV9lrbgRuBDyWZZbCP//b2lNuBN7b6h4CtY/RbkjSmsa4GWlW3ALcsKD8HXDSi7d8A14yzPknS6vGbwJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOjVWACQ5I8mnk3w1yZ4k/yjJWUl2JXm2PZ7Z2ibJJ5LMJnkyyYWr8xYkSSsx7gjg48B/q6p/APxDYA+Dm70/XFUbgYc5dPP3K4GN7WcLcNuY65YkjWHFN4VP8kPAPwP+DUBVvQ68nmQTcElrdifwBeBGYBNwV1UV8GgbPZxbVS+tuPfSmKa3PjDpLkgTM84I4EeBeeA/J/mzJJ9K8gPAmw5+qLfHc1r7dcDeoefPtZokaQLGCYBTgQuB26rq7cD/5dDunlEyolaHNUq2JNmdZPf8/PwY3ZMkHck4ATAHzFXVY23+0wwC4eUk5wK0x31D7TcMPX898OLCF62q7VU1U1UzU1NTY3RPknQkKw6AqvpLYG+Sn2ilS4FngJ3A5lbbDNzfpncC72tnA10MvOr+f0manBUfBG5+Bbg7yWnAc8D7GYTKfUmuB14ArmltHwSuAmaB11pbSdKEjBUAVfVlYGbEoktHtC3ghnHWJ0laPeOOACQtw8LTTZ/fdvWEeiId4qUgJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVNjB0CSU5L8WZL/2ubPT/JYkmeT/H67XzBJTm/zs2359LjrliSt3GqMAD4I7Bma/yhwa1VtBF4Brm/164FXqurHgFtbO0nShIx1T+Ak64GrgY8AH0oS4J3Az7UmdwL/HrgN2NSmAT4N/G6StJvFS2ti4b15pZ6NOwL4HeDDwHfb/BuBb1XVgTY/B6xr0+uAvQBt+aut/fdIsiXJ7iS75+fnx+yeJGkxKw6AJO8G9lXVE8PlEU1rGcsOFaq2V9VMVc1MTU2ttHuSpCWMswvop4H3JLkK+H7ghxiMCM5Icmr7K3898GJrPwdsAOaSnAr8MLB/jPVLksaw4hFAVd1UVeurahq4FvhcVf088HngZ1uzzcD9bXpnm6ct/5z7/yVpco7F9wBuZHBAeJbBPv7bW/124I2t/iFg6zFYtyRpmcY6C+igqvoC8IU2/Rxw0Yg2fwNcsxrrkySNz28CS1KnVmUEIOnoLPw+wvPbrp5QT9QzRwCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQXg9NJzZvAS4tzBCBJnTIAJKlTKw6AJBuSfD7JniRPJ/lgq5+VZFeSZ9vjma2eJJ9IMpvkySQXrtabkCQdvXFGAAeAX6+qnwQuBm5IcgGDe/0+XFUbgYc5dO/fK4GN7WcLcNsY65YkjWnFB4Gr6iXgpTb910n2AOuATcAlrdmdDO4VfGOr31VVBTya5Iwk57bXkbrmHcI0CatyDCDJNPB24DHgTQc/1NvjOa3ZOmDv0NPmWk2SNAFjB0CSHwT+APi1qvqrIzUdUasRr7clye4ku+fn58ftniRpEWN9DyDJ9zH48L+7qj7Tyi8f3LWT5FxgX6vPARuGnr4eeHHha1bVdmA7wMzMzGEBIR2J5/1LyzfOWUABbgf2VNXHhhbtBDa36c3A/UP197WzgS4GXnX/vyRNzjgjgJ8G/jXwlSRfbrXfALYB9yW5HngBuKYtexC4CpgFXgPeP8a6JUljGucsoP/J6P36AJeOaF/ADStdnyRpdflNYEnqlAEgSZ3yaqA6oZ2sZ/0Mvy+/FKZjxRGAJHXKAJCkThkAktQpA0CSOmUASFKnPAtIOs55qWgdK44AJKlTjgB03DtZz/WXJs0RgCR1yhGAdILxmIBWiyMASeqUIwAdd9znL60NA0A6wblLSCtlAGji/ItfmgwDQGvOD/xjyxGBlmvNAyDJFcDHgVOAT1XVtrXug449P+SPH0f6tzAc+ramAZDkFOCTwLuAOeDxJDur6pm17IekAUcLfVvrEcBFwGxVPQeQ5F5gE2AAHAf8q10LGRAnt7UOgHXA3qH5OeAda9yHiVvNIbkf2lpNS/0+HS+/b0v9Pzma/2M9h9xaB0BG1Op7GiRbgC1t9v8k+dox79WRnQ18c61Wlo+u1ZpWZE23xXHM7XDIRLbFOP9PlnruCl/7ePud+HvLabTWATAHbBiaXw+8ONygqrYD29eyU0eSZHdVzUy6H8cDt8WA2+EQt8XAibod1vpSEI8DG5Ocn+Q04Fpg5xr3QZLEGo8AqupAkg8ADzE4DXRHVT29ln2QJA2s+fcAqupB4MG1Xu8YjpvdUccBt8WA2+EQt8XACbkdUlVLt5IknXS8HLQkdcoAWESS307y1SRPJvnDJGcMLbspyWySryW5fJL9PNaSXJPk6STfTTKzYFk32+GgJFe09zubZOuk+7OWkuxIsi/JU0O1s5LsSvJsezxzkn1cC0k2JPl8kj3t/8YHW/2E2xYGwOJ2AW+tqrcBfwHcBJDkAgZnL70FuAL4vXaJi5PVU8C/Ah4ZLna4HYYvZXIlcAFwXdsOvbiDwb/1sK3Aw1W1EXi4zZ/sDgC/XlU/CVwM3NB+D064bWEALKKq/riqDrTZRxl8ZwEGl664t6q+XVVfB2YZXOLipFRVe6pq1JfxutoOzd9eyqSqXgcOXsqkC1X1CLB/QXkTcGebvhN475p2agKq6qWq+lKb/mtgD4OrHJxw28IAWJ5fAD7bpkddzmLdmvdo8nrcDj2+56W8qapegsEHI3DOhPuzppJMA28HHuME3BZd3w8gyZ8APzJi0c1VdX9rczODId/dB582ov0JfSrVcrbDqKeNqJ3Q22EZenzPWkSSHwT+APi1qvqrZNSvx/Gt6wCoqp850vIkm4F3A5fWofNll7ycxYlmqe2wiJNuOyxDj+95KS8nObeqXkpyLrBv0h1aC0m+j8GH/91V9ZlWPuG2hbuAFtFuXHMj8J6qem1o0U7g2iSnJzkf2Ah8cRJ9nLAet4OXMjncTmBzm94MLDZiPGlk8Kf+7cCeqvrY0KITblv4RbBFJJkFTgf+dys9WlW/2JbdzOC4wAEGw7/Pjn6VE1+Sfwn8R2AK+Bbw5aq6vC3rZjsclOQq4Hc4dCmTj0y4S2smyT3AJQyufPkycAvwR8B9wHnAC8A1VbXwQPFJJck/Af4H8BXgu638GwyOA5xQ28IAkKROuQtIkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1Kn/D+KGheh8yJiHAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(actual - predictions, bins = 80); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9594218638137143"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "\n",
    "sklearn.metrics.r2_score(predictions, actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.     , 0.98213],\n",
       "       [0.98213, 1.     ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(predictions, actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save predictions\n",
    "predictions_df = pd.DataFrame(np.hstack((results[-1], dataset[2].reshape((-1,1)), idx_split)), columns=['Prediction', 'Actual', 'idx_split'])\n",
    "predictions_df.index = np.array(keys[0], dtype = int)\n",
    "predictions_df.to_csv('PredictionsNoAuxilarySemisupervised.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
